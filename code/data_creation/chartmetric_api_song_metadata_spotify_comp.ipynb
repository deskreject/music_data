{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t# Author: Alexander Staub\n",
    "\t## Last changed: 2025.02.18\n",
    "\t## Purpose: Using the chartmetric IDs to get song level metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing packages\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the logging of the errors\n",
    "logging.basicConfig(\n",
    "    filename='chartmetric_api_metadata.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s: %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define API host and your refresh token\n",
    "HOST = 'https://api.chartmetric.com'\n",
    "with open(\"chartmetric_refresh_token.txt\", \"r\") as f:\n",
    "    REFRESH_TOKEN = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve an access token using the refresh token\n",
    "token_response = requests.post(f'{HOST}/api/token', json={'refreshtoken': REFRESH_TOKEN})\n",
    "\n",
    "# Check if the token was retrieved successfully\n",
    "if token_response.status_code != 200:\n",
    "\n",
    "    # Log the error and raise an exception\n",
    "    logging.error(f\"Token retrieval error: {token_response.status_code}\")\n",
    "    raise Exception(f\"Error: received {token_response.status_code} from /api/token\")\n",
    "\n",
    "# Extract the access token from the response\n",
    "access_token = token_response.json()['token']\n",
    "\n",
    "# Define the headers for the API requests\n",
    "headers = {'Authorization': f'Bearer {access_token}'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the get_request\n",
    "\n",
    "Robust request logic that:\n",
    "- backs off for a max of 26 hours in retries\n",
    "- logs all erros it encounters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Robust get_request Function ---\n",
    "def get_request(endpoint, params=None, max_retries=5):\n",
    "    backoff = 1  # initial backoff in seconds (used if header data is missing)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(f\"{HOST}{endpoint}\", headers=headers, params=params)\n",
    "        except Exception as ex:\n",
    "            logging.error(f\"Network error on attempt {attempt+1} for {endpoint}: {ex}\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "            continue\n",
    "\n",
    "# Log the response status code and rate limit headers\n",
    "        logging.info(f\"Request to {endpoint} returned {response.status_code}. RateLimit headers: {response.headers}\")\n",
    "\n",
    "# Check if the response status code is 200\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "\n",
    "# Handle different types of errors\n",
    "# 401: Token may have expired; refresh it\n",
    "        elif response.status_code == 401:\n",
    "            # Token may have expired; refresh it\n",
    "            logging.warning(f\"401 error for {endpoint}. Refreshing token.\")\n",
    "            token_response = requests.post(f'{HOST}/api/token', json={'refreshtoken': REFRESH_TOKEN})\n",
    "            if token_response.status_code != 200:\n",
    "                logging.error(f\"Token refresh failed: {token_response.status_code}\")\n",
    "                raise Exception(f\"Token refresh failed with status {token_response.status_code}\")\n",
    "            new_token = token_response.json()['token']\n",
    "            headers['Authorization'] = f'Bearer {new_token}'\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "\n",
    "# 429: Rate limit exceeded; wait and retry\n",
    "        elif response.status_code == 429:\n",
    "            # Rate limit exceeded.\n",
    "            reset_timestamp = response.headers.get(\"X-RateLimit-Reset\")\n",
    "            if reset_timestamp:\n",
    "                # Wait until the time provided by the API\n",
    "                sleep_time = int(reset_timestamp) - int(time.time())\n",
    "                if sleep_time < 0:\n",
    "                    sleep_time = backoff\n",
    "            else:\n",
    "                # No wait time provided by the API; compute one that totals 26 hours over all retries.\n",
    "                total_wait_limit = 26 * 3600  # total wait time in seconds (26 hours)\n",
    "                # Sum exponential weights for remaining attempts: for i from current attempt to max_retries-1\n",
    "                remaining_weights = sum(2 ** i for i in range(attempt, max_retries))\n",
    "                # Use the weight for the current attempt to assign a fraction of the total wait.\n",
    "                sleep_time = total_wait_limit * (2 ** attempt / remaining_weights)\n",
    "            logging.warning(f\"429 error for {endpoint}. Sleeping for {sleep_time} seconds (attempt {attempt+1}/{max_retries}).\")\n",
    "            time.sleep(sleep_time)\n",
    "            backoff *= 2\n",
    "\n",
    "# 500: Server error; wait and retry\n",
    "        elif response.status_code >= 500:\n",
    "            logging.warning(f\"Server error {response.status_code} for {endpoint}. Retrying after {backoff} seconds.\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "\n",
    "        else:\n",
    "            logging.error(f\"Error {response.status_code} for {endpoint}: {response.text}\")\n",
    "            raise Exception(f\"Error: received {response.status_code} from {endpoint}\")\n",
    "\n",
    "# If the loop completes without returning, raise an exception\n",
    "    raise Exception(f\"Max retries exceeded for endpoint {endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use chartmetric ID to access song characteristics:\n",
    "- use the chartmetric ID file to get the songs for which we have chartmetric ID\n",
    "- loop over chartmetric id to access the track metadata endpoint and retreive song level of relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# the different lists of ids\n",
    "chartmetric_ids_spotify_1 = pd.read_csv(\"Z:/Data_alexander/data/incidental/chartmetric/chartmetric_ids_spotify_sample_1.csv\")\n",
    "chartmetric_ids_spotify_2 = pd.read_csv(\"Z:/Data_alexander/data/incidental/chartmetric/chartmetric_ids_spotify_sample_2.csv\")\n",
    "chartmetric_ids_spotify_3 = pd.read_csv(\"Z:/Data_alexander/data/incidental/chartmetric/chartmetric_ids_spotify_sample_3.csv\")\n",
    "\n",
    "# concatenate the dataframes into one\n",
    "chartmetric_ids_spotify = pd.concat([chartmetric_ids_spotify_1, chartmetric_ids_spotify_2, chartmetric_ids_spotify_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retain only the rows with unique chartmetric_ids\n",
    "chartmetric_ids_spotify = chartmetric_ids_spotify.drop_duplicates(subset=\"chartmetric_ids\")\n",
    "\n",
    "#retain only 10 rows of the dataframe\n",
    "#chartmetric_ids_spotify = chartmetric_ids_spotify.head(10)\n",
    "\n",
    "# Reset the index to ensure unique indexing\n",
    "chartmetric_ids_spotify.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to retreive the song metadata from chartmetrics\n",
    "\n",
    "- create the get request\n",
    "- run the loop over each chartmetric id\n",
    "- save the response for later parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to Retrieve song characteristics from Chartmetric ID ---\n",
    "def get_songchars_ids(chartmetric_id):\n",
    "    endpoint = f\"/api/track/{chartmetric_id}\"\n",
    "    try:\n",
    "        response = get_request(endpoint)\n",
    "        logging.info(f\"Successfully retrieved song chars for Chartmetric ID {chartmetric_id}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to get song chars for Chartmetric ID {chartmetric_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # The API response (a dictionary) is returned as is\n",
    "    song_chars = response\n",
    "    return song_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0: Chartmetric ID = 15812884.0\n",
      "Row 0 processed: Chartmetric ID = 15812884.0\n",
      "Processing row 1: Chartmetric ID = 15440434.0\n",
      "Row 1 processed: Chartmetric ID = 15440434.0\n",
      "Processing row 2: Chartmetric ID = 20706768.0\n",
      "Row 2 processed: Chartmetric ID = 20706768.0\n",
      "Processing row 3: Chartmetric ID = 12820755.0\n",
      "Row 3 processed: Chartmetric ID = 12820755.0\n",
      "Processing row 4: Chartmetric ID = 15447513.0\n",
      "Row 4 processed: Chartmetric ID = 15447513.0\n",
      "Processing row 5: Chartmetric ID = 15440175.0\n",
      "Row 5 processed: Chartmetric ID = 15440175.0\n",
      "Processing row 6: Chartmetric ID = 12486440.0\n",
      "Row 6 processed: Chartmetric ID = 12486440.0\n",
      "Processing row 7: Chartmetric ID = 44970155.0\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Loop over the DataFrame to retrieve API responses ---\n",
    "# Assume spotify_sample is your existing DataFrame.\n",
    "# The responses will be stored in the list below.\n",
    "song_chars_responses = []  # List to store API responses for each Chartmetric ID.\n",
    "checkpoint_interval = 100  # Save a checkpoint every 100 processed rows.\n",
    "checkpoint_file = \"Z:/Data_alexander/data/incidental/chartmetric/song_chars_checkpoint.json\"  # Checkpoint file for responses.\n",
    "\n",
    "for idx, row in chartmetric_ids_spotify.iterrows():\n",
    "    chartmetric_id = row.get(\"chartmetric_ids\")\n",
    "    print(f\"Processing row {idx}: Chartmetric ID = {chartmetric_id}\")\n",
    "    logging.info(f\"Processing row {idx}: Chartmetric ID = {chartmetric_id}\")\n",
    "    \n",
    "    if pd.isnull(chartmetric_id):\n",
    "        print(f\"Row {idx} has no Chartmetric ID. Skipping.\")\n",
    "        logging.info(f\"Row {idx} has no Chartmetric ID. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        song_chars = get_songchars_ids(chartmetric_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Chartmetric ID {chartmetric_id} at row {idx}: {e}\")\n",
    "        logging.error(f\"Error processing Chartmetric ID {chartmetric_id} at row {idx}: {e}\")\n",
    "        song_chars = None\n",
    "    \n",
    "    # Append the response (or None) to our list.\n",
    "    song_chars_responses.append(song_chars)\n",
    "    \n",
    "    print(f\"Row {idx} processed: Chartmetric ID = {chartmetric_id}\")\n",
    "    logging.info(f\"Processed row {idx}: Chartmetric ID = {chartmetric_id}\")\n",
    "    \n",
    "    # Sleep briefly to help with rate limiting.\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    # Save a checkpoint periodically.\n",
    "    if idx % checkpoint_interval == 0 and idx > 0:\n",
    "        with open(checkpoint_file, \"w\") as f:\n",
    "            json.dump(song_chars_responses, f, indent=2)\n",
    "        print(f\"Checkpoint saved at row {idx}\")\n",
    "        logging.info(f\"Checkpoint saved at row {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to extract necessary information from the search output\n",
    "def extract_song_info(search_output):\n",
    "    # Extract the main object\n",
    "    obj = search_output.get('obj', {})\n",
    "    \n",
    "    # Artist: take first artist if available\n",
    "    if obj.get('artists') and len(obj['artists']) > 0:\n",
    "        artist = obj['artists'][0]\n",
    "        artist_id = artist.get('id', None)\n",
    "        artist_name = artist.get('name', None)\n",
    "        artist_label = artist.get('label', None)\n",
    "        artist_booking_agent = artist.get('booking_agent', None)\n",
    "        artist_general_manager = artist.get('general_manager', None)\n",
    "    else:\n",
    "        artist_id = artist_name = artist_label = artist_booking_agent = artist_general_manager = None\n",
    "\n",
    "    # Albums: select the album with the earliest release date\n",
    "    if obj.get('albums') and len(obj['albums']) > 0:\n",
    "        def parse_date(album):\n",
    "            try:\n",
    "                return datetime.strptime(album.get('release_date', ''), '%Y-%m-%d')\n",
    "            except Exception:\n",
    "                return datetime.max\n",
    "        sorted_albums = sorted(obj['albums'], key=parse_date)\n",
    "        earliest_album = sorted_albums[0]\n",
    "        album_id = earliest_album.get('id', None)\n",
    "        album_name = earliest_album.get('name', None)\n",
    "        album_release_date = earliest_album.get('release_date', None)\n",
    "        album_label = earliest_album.get('label', None)\n",
    "    else:\n",
    "        album_id = album_name = album_release_date = album_label = None\n",
    "\n",
    "    # Use a pipe '|' as delimiter for multiple values\n",
    "    delimiter = '|'\n",
    "    \n",
    "    # Moods: concatenate mood names\n",
    "    if obj.get('moods') and len(obj['moods']) > 0:\n",
    "        moods = delimiter.join([m.get('name', '') for m in obj['moods']])\n",
    "    else:\n",
    "        moods = None\n",
    "    \n",
    "    # Activities: concatenate activity names\n",
    "    if obj.get('activities') and len(obj['activities']) > 0:\n",
    "        activities = delimiter.join([a.get('name', '') for a in obj['activities']])\n",
    "    else:\n",
    "        activities = None\n",
    "    \n",
    "    # Songwriters: concatenate songwriter names\n",
    "    if obj.get('songwriters') and len(obj['songwriters']) > 0:\n",
    "        songwriters = delimiter.join(obj['songwriters'])\n",
    "    else:\n",
    "        songwriters = None\n",
    "    \n",
    "    # songwriterIds is not present in the example so we assign None\n",
    "    songwriterIds = None\n",
    "\n",
    "    # Create a one-row DataFrame with the desired columns\n",
    "    data = {\n",
    "        'chartmetric_ids': obj.get('id', None),\n",
    "        'Name': obj.get('name', None),\n",
    "        'Composer_name': obj.get('composer_name', None),\n",
    "        'Artist_id': artist_id,\n",
    "        'Artist_name': artist_name,\n",
    "        'Artist_label': artist_label,\n",
    "        'Artist_booking_agent': artist_booking_agent,\n",
    "        'Artist_general_manager': artist_general_manager,\n",
    "        'Albums_id': album_id,\n",
    "        'Albums_name': album_name,\n",
    "        'Albums_release_date': album_release_date,\n",
    "        'Albums_label': album_label,\n",
    "        'Tags': obj.get('tags', None),\n",
    "        'Moods': moods,\n",
    "        'Activities': activities,\n",
    "        'Songwriters': songwriters,\n",
    "        'songwriterIds': songwriterIds,\n",
    "        'Tempo': obj.get('tempo', None),\n",
    "        'Duration_ms': obj.get('duration_ms', None)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_7044\\3021864179.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  song_chars_extracted = pd.concat(extracted_rows, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Parse the Collected API Responses ---\n",
    "# Assume the extraction function 'extract_song_info' is defined (as provided previously).\n",
    "extracted_rows = []\n",
    "for resp in song_chars_responses:\n",
    "    if resp is not None:\n",
    "        # Extract song information from the response.\n",
    "        extracted_df = extract_song_info(resp)\n",
    "        extracted_rows.append(extracted_df)\n",
    "\n",
    "if extracted_rows:\n",
    "    song_chars_extracted = pd.concat(extracted_rows, ignore_index=True)\n",
    "else:\n",
    "    song_chars_extracted = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Join the Extracted Data to the Original DataFrame ---\n",
    "# It is assumed that the 'id' column in song_chars_extracted matches the 'id' column in spotify_sample.\n",
    "merged_song_chars = chartmetric_ids_spotify.merge(song_chars_extracted, on=\"chartmetric_ids\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safe the final dataframe\n",
    "# Save as JSON (records-oriented with one JSON object per line)\n",
    "merged_song_chars.to_json(\"Z:/Data_alexander/data/incidental/chartmetric/final_joined_data.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code used to derive an example and extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to Retrieve song characteristics from Chartmetric ID ---\n",
    "def get_songchars_ids(chartmetric_id):\n",
    "    endpoint = f\"/api/track/{chartmetric_id}\"\n",
    "    try:\n",
    "        response = get_request(endpoint)\n",
    "        # Log the response status code and rate limit headers\n",
    "        logging.info(f\"Successfully retrieved song chars for chartmetric id {chartmetric_id}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to get song chars for chartmetric id {chartmetric_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Extract the song characteristics from the response\n",
    "    song_chars = response\n",
    "\n",
    "    return song_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15447513.0\n"
     ]
    }
   ],
   "source": [
    "pprint(chartmetric_ids_spotify[\"chartmetric_ids\"][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial run with a single ID \n",
    "\n",
    "search_output = get_songchars_ids(chartmetric_ids_spotify[\"chartmetric_ids\"][0])\n",
    "\n",
    "\n",
    "pprint(search_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trial the function with the search output\n",
    "\n",
    "test_df = extract_song_info(search_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_data_chartmetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
