{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Alexander Staub\n",
    "* **Last changed:** 2025.06.29\n",
    "* **Purpose:** This notebook is the final step in the Chartmetric ID retrieval process. It runs **once** after all worker scripts have completed.\n",
    "    1.  It automatically finds all worker checkpoint files.\n",
    "    2.  It concatenates them into a single, complete file of results, handling duplicates created by the copy-paste setup.\n",
    "    3.  It loads the original master dataset (with all metadata columns).\n",
    "    4.  It performs a left merge to add the `chartmetric_ids` to the master dataset.\n",
    "    5.  It saves the final, enriched dataset to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing packages\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Full Original Dataset ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m CHECKPOINT_FILE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/chartmetric_track_id_checkpoints/chartmetric_ids_checkpoint.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Loading Full Original Dataset ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading the master dataset from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mORIGINAL_DATA_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Your original loading and cleaning logic\u001b[39;00m\n\u001b[0;32m     16\u001b[0m spotify_album \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(ORIGINAL_DATA_FILE)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logging' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "# Define the paths and parameters for the merge process.\n",
    "\n",
    "# The base directory where the worker output parts are stored.\n",
    "# This should match the ID_OUTPUT_DIR from your worker scripts.\n",
    "WORKER_OUTPUT_DIR = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/\"\n",
    "\n",
    "# The location of your original, full dataset (the one the controller used).\n",
    "ORIGINAL_MASTER_INPUT_FILE = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/Spotify/1980_2000_songs_artists/musicbrainz_spotify_combined_track_artist_final.csv\"\n",
    "\n",
    "# CHANGE the path for the chart songs\n",
    "# ORIGINAL_MASTER_INPUT_FILE =\n",
    "\n",
    "# The path where the single, combined checkpoint file will be saved.\n",
    "FINAL_CONSOLIDATED_CHECKPOINT = os.path.join(WORKER_OUTPUT_DIR, \"chartmetric_ids_complete_checkpoint.csv\")\n",
    "\n",
    "# CHANGE the path for the chart songs\n",
    "# FINAL_OUTPUT_FILE_WITH_IDS = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/chartmetric_ids_chart_songs_matched.csv\"\n",
    "\n",
    "print(\"Configuration set.\")\n",
    "print(f\"Worker output directory: {WORKER_OUTPUT_DIR}\")\n",
    "print(f\"Final output file will be: {FINAL_OUTPUT_FILE_WITH_IDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Find and Consolidate All Worker Checkpoint Files\n",
    "\n",
    "This step finds all the `chartmetric_ids_checkpoint.csv` files inside the `part_*` subdirectories, loads them into a list of dataframes, and concatenates them into a single dataframe.\n",
    "\n",
    "**Important:** Because of the \"copy-paste\" setup method, each worker checkpoint contains the original ~180k rows plus its own new results. This step **must** drop duplicates to create a clean, final list of all unique ISRCs that have been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use glob to automatically find all worker checkpoint files\n",
    "search_pattern = os.path.join(WORKER_OUTPUT_DIR, \"part_*\", \"chartmetric_ids_checkpoint.csv\")\n",
    "worker_checkpoint_files = glob.glob(search_pattern)\n",
    "\n",
    "if not worker_checkpoint_files:\n",
    "    raise FileNotFoundError(f\"No worker checkpoint files were found using the pattern: {search_pattern}. Please ensure the workers have run and the WORKER_OUTPUT_DIR is correct.\")\n",
    "\n",
    "print(f\"Found {len(worker_checkpoint_files)} worker checkpoint files:\")\n",
    "for f in worker_checkpoint_files:\n",
    "    print(f\" - {f}\")\n",
    "\n",
    "# Load all checkpoint files into a list of dataframes\n",
    "all_worker_dfs = [pd.read_csv(f) for f in worker_checkpoint_files]\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "print(\"\\nConcatenating all worker files...\")\n",
    "consolidated_df = pd.concat(all_worker_dfs, ignore_index=True)\n",
    "print(f\"Total rows before duplicate removal: {len(consolidated_df):,}\")\n",
    "\n",
    "# CRITICAL STEP: Drop duplicates based on 'spotify_isrc'\n",
    "# This handles the overlap from the initial copy-paste and any potential runtime overlaps.\n",
    "consolidated_df.drop_duplicates(subset=['spotify_isrc'], keep='first', inplace=True)\n",
    "print(f\"Total unique rows after duplicate removal: {len(consolidated_df):,}\")\n",
    "\n",
    "# Save the final, consolidated checkpoint file\n",
    "consolidated_df.to_csv(FINAL_CONSOLIDATED_CHECKPOINT, index=False)\n",
    "print(f\"\\nSuccessfully saved the complete, consolidated checkpoint file to:\\n{FINAL_CONSOLIDATED_CHECKPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Merge Chartmetric IDs into the Master Dataset\n",
    "\n",
    "Now, we load the original dataset with all its rich metadata and the complete list of Chartmetric IDs we just created. We then perform a `left` merge to add the `chartmetric_ids` column to the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original master dataset\n",
    "print(f\"Loading original master data from:\\n{ORIGINAL_MASTER_INPUT_FILE}\")\n",
    "master_df = pd.read_csv(ORIGINAL_MASTER_INPUT_FILE)\n",
    "print(f\"Loaded {len(master_df):,} rows from the master file.\")\n",
    "\n",
    "# Load the consolidated chartmetric IDs we just created\n",
    "print(f\"\\nLoading consolidated Chartmetric IDs from:\\n{FINAL_CONSOLIDATED_CHECKPOINT}\")\n",
    "chartmetric_ids_df = pd.read_csv(FINAL_CONSOLIDATED_CHECKPOINT)\n",
    "print(f\"Loaded {len(chartmetric_ids_df):,} unique Chartmetric ID results.\")\n",
    "\n",
    "\n",
    "# Perform the left merge to add the new IDs to the master dataframe\n",
    "print(\"\\nMerging Chartmetric IDs into the master dataframe...\")\n",
    "spotify_fetch = pd.merge(master_df, chartmetric_ids_df, on='spotify_isrc', how='left')\n",
    "print(\"Merge complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the object type into float \n",
    "spotify_fetch[\"chartmetric_ids\"] = pd.to_numeric(spotify_fetch[\"chartmetric_ids\"], errors='coerce')\n",
    "\n",
    "#transform into integer\n",
    "spotify_fetch[\"chartmetric_ids\"] = spotify_fetch[\"chartmetric_ids\"].astype(\"Int64\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retain only the rows with unique chartmetric_ids\n",
    "spotify_fetch_unique = spotify_fetch.drop_duplicates(subset=\"chartmetric_ids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the files needs to take into account the version that I have already saved in the past as I am not able to run the code remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file as: //bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/chartmetric_ids_sample.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Define the final file path\n",
    "# NEED TO CHECK:  the suffix\n",
    "filepath = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/\"\n",
    "\n",
    "#give the file a name\n",
    "file_name = \"chartmetric_ids_mb_matched.csv\"\n",
    "\n",
    "#for chart songs\n",
    "# file_name = \"chartmetric_ids_chart_songs_matched.csv\"\n",
    "\n",
    "\n",
    "#paste filepath and file_name together and call the variable final_filepath\n",
    "\n",
    "final_filepath = os.path.join(filepath, file_name)\n",
    "\n",
    "# Save the dataframe to the final_filepath\n",
    "spotify_fetch_unique.to_csv(final_filepath, index=False)\n",
    "print(f\"Saved file as: {final_filepath}\")\n",
    "\n",
    "# Log the final message in the log file\n",
    "logging.info(\"Completed processing all ISRC codes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_data_chartmetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
