{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Alexander Staub\n",
    "* **Last changed:** 2025.06.29\n",
    "* **Purpose:** This notebook is the final step in the Chartmetric ID retrieval process. It runs **once** after all worker scripts have completed.\n",
    "    1.  It automatically finds all worker checkpoint files.\n",
    "    2.  It concatenates them into a single, complete file of results, handling duplicates created by the copy-paste setup.\n",
    "    3.  It loads the original master dataset (with all metadata columns).\n",
    "    4.  It performs a left merge to add the `chartmetric_ids` to the master dataset.\n",
    "    5.  It saves the final, enriched dataset to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing packages\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "#load package for glob function\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set.\n",
      "Worker output directory: //bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'FINAL_OUTPUT_FILE_WITH_IDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfiguration set.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorker output directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWORKER_OUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal output file will be: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFINAL_OUTPUT_FILE_WITH_IDS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FINAL_OUTPUT_FILE_WITH_IDS' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "# Define the paths and parameters for the merge process.\n",
    "\n",
    "# The base directory where the worker output parts are stored.\n",
    "# This should match the ID_OUTPUT_DIR from your worker scripts.\n",
    "WORKER_OUTPUT_DIR = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/\"\n",
    "\n",
    "# The location of your original, full dataset (the one the controller used).\n",
    "ORIGINAL_MASTER_INPUT_FILE = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/Spotify/1980_2000_songs_artists/final_charted_from_spotify_1980_2000.csv\"\n",
    "\n",
    "# CHANGE the path for the chart songs\n",
    "# ORIGINAL_MASTER_INPUT_FILE =\n",
    "\n",
    "# The path where the single, combined checkpoint file will be saved.\n",
    "FINAL_CONSOLIDATED_CHECKPOINT = os.path.join(WORKER_OUTPUT_DIR, \"chartmetric_ids_complete_checkpoint.csv\")\n",
    "\n",
    "# CHANGE the path for the chart songs\n",
    "# FINAL_OUTPUT_FILE_WITH_IDS = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/chartmetric_ids_chart_songs_matched.csv\"\n",
    "\n",
    "print(\"Configuration set.\")\n",
    "print(f\"Worker output directory: {WORKER_OUTPUT_DIR}\")\n",
    "print(f\"Final output file will be: {FINAL_OUTPUT_FILE_WITH_IDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Find and Consolidate All Worker Checkpoint Files\n",
    "\n",
    "This step finds all the `chartmetric_ids_checkpoint.csv` files inside the `part_*` subdirectories, loads them into a list of dataframes, and concatenates them into a single dataframe.\n",
    "\n",
    "**Important:** Because of the \"copy-paste\" setup method, each worker checkpoint contains the original ~180k rows plus its own new results. This step **must** drop duplicates to create a clean, final list of all unique ISRCs that have been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 worker checkpoint files:\n",
      " - //bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids\\part_3\\chartmetric_ids_checkpoint.csv\n",
      " - //bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids\\part_2\\chartmetric_ids_checkpoint.csv\n",
      " - //bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids\\part_1\\chartmetric_ids_checkpoint.csv\n",
      "\n",
      "Concatenating all worker files...\n",
      "Total rows before duplicate removal: 24,056\n",
      "Total unique rows after duplicate removal: 24,056\n",
      "\n",
      "Successfully saved the complete, consolidated checkpoint file to:\n",
      "//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/chartmetric_ids_complete_checkpoint.csv\n"
     ]
    }
   ],
   "source": [
    "# Use glob to automatically find all worker checkpoint files\n",
    "search_pattern = os.path.join(WORKER_OUTPUT_DIR, \"part_*\", \"chartmetric_ids_checkpoint.csv\")\n",
    "worker_checkpoint_files = glob.glob(search_pattern)\n",
    "\n",
    "if not worker_checkpoint_files:\n",
    "    raise FileNotFoundError(f\"No worker checkpoint files were found using the pattern: {search_pattern}. Please ensure the workers have run and the WORKER_OUTPUT_DIR is correct.\")\n",
    "\n",
    "print(f\"Found {len(worker_checkpoint_files)} worker checkpoint files:\")\n",
    "for f in worker_checkpoint_files:\n",
    "    print(f\" - {f}\")\n",
    "\n",
    "# Load all checkpoint files into a list of dataframes\n",
    "all_worker_dfs = [pd.read_csv(f) for f in worker_checkpoint_files]\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "print(\"\\nConcatenating all worker files...\")\n",
    "consolidated_df = pd.concat(all_worker_dfs, ignore_index=True)\n",
    "print(f\"Total rows before duplicate removal: {len(consolidated_df):,}\")\n",
    "\n",
    "# CRITICAL STEP: Drop duplicates based on 'spotify_isrc'\n",
    "# This handles the overlap from the initial copy-paste and any potential runtime overlaps.\n",
    "consolidated_df.drop_duplicates(subset=['spotify_isrc'], keep='first', inplace=True)\n",
    "print(f\"Total unique rows after duplicate removal: {len(consolidated_df):,}\")\n",
    "\n",
    "# Save the final, consolidated checkpoint file\n",
    "consolidated_df.to_csv(FINAL_CONSOLIDATED_CHECKPOINT, index=False)\n",
    "print(f\"\\nSuccessfully saved the complete, consolidated checkpoint file to:\\n{FINAL_CONSOLIDATED_CHECKPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Merge Chartmetric IDs into the Master Dataset\n",
    "\n",
    "Now, we load the original dataset with all its rich metadata and the complete list of Chartmetric IDs we just created. We then perform a `left` merge to add the `chartmetric_ids` column to the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original master data from:\n",
      "//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/Spotify/1980_2000_songs_artists/final_charted_from_spotify_1980_2000.csv\n",
      "Loaded 24,056 rows from the master file.\n",
      "\n",
      "Loading consolidated Chartmetric IDs from:\n",
      "//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/chartmetric_ids_complete_checkpoint.csv\n",
      "Loaded 24,056 unique Chartmetric ID results.\n",
      "\n",
      "Merging Chartmetric IDs into the master dataframe...\n",
      "Merge complete.\n"
     ]
    }
   ],
   "source": [
    "# Load the original master dataset\n",
    "print(f\"Loading original master data from:\\n{ORIGINAL_MASTER_INPUT_FILE}\")\n",
    "master_df = pd.read_csv(ORIGINAL_MASTER_INPUT_FILE)\n",
    "print(f\"Loaded {len(master_df):,} rows from the master file.\")\n",
    "\n",
    "# Load the consolidated chartmetric IDs we just created\n",
    "print(f\"\\nLoading consolidated Chartmetric IDs from:\\n{FINAL_CONSOLIDATED_CHECKPOINT}\")\n",
    "chartmetric_ids_df = consolidated_df.copy()\n",
    "print(f\"Loaded {len(chartmetric_ids_df):,} unique Chartmetric ID results.\")\n",
    "\n",
    "\n",
    "# Perform the left merge to add the new IDs to the master dataframe\n",
    "print(\"\\nMerging Chartmetric IDs into the master dataframe...\")\n",
    "spotify_fetch = pd.merge(master_df, chartmetric_ids_df, on='spotify_isrc', how='left')\n",
    "print(\"Merge complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the object type into float \n",
    "spotify_fetch[\"chartmetric_ids\"] = pd.to_numeric(spotify_fetch[\"chartmetric_ids\"], errors='coerce')\n",
    "\n",
    "#transform into integer\n",
    "spotify_fetch[\"chartmetric_ids\"] = spotify_fetch[\"chartmetric_ids\"].astype(\"Int64\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the missing entries of chartmetric_ids\n",
    "spotify_fetch = spotify_fetch[spotify_fetch[\"chartmetric_ids\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the duplicate ISRCs\n",
    "spotify_fetch.drop_duplicates(subset=['spotify_isrc'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the files needs to take into account the version that I have already saved in the past as I am not able to run the code remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file as: //bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/chartmetric_ids_chart_songs_matched.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Define the final file path\n",
    "# NEED TO CHECK:  the suffix\n",
    "filepath = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/\"\n",
    "\n",
    "#give the file a name\n",
    "# file_name = \"chartmetric_ids_mb_matched.csv\"\n",
    "\n",
    "#for chart songs\n",
    "file_name = \"chartmetric_ids_chart_songs_matched.csv\"\n",
    "\n",
    "\n",
    "#paste filepath and file_name together and call the variable final_filepath\n",
    "\n",
    "final_filepath = os.path.join(filepath, file_name)\n",
    "\n",
    "# Save the dataframe to the final_filepath\n",
    "spotify_fetch.to_csv(final_filepath, index=False)\n",
    "print(f\"Saved file as: {final_filepath}\")\n",
    "\n",
    "# Log the final message in the log file\n",
    "logging.info(\"Completed processing all ISRC codes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks of the consolidated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the percentage of missing chartmetric_ids in the consolidated_df dataframe?\n",
    "missing_percentage = (consolidated_df['chartmetric_ids'].isnull().sum() / len(consolidated_df)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique chartmetric_ids are in the consolidated_df dataframe?\n",
    "unique_chartmetric_ids_count = consolidated_df['chartmetric_ids'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using spotify_fetch dataframe, what "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_data_chartmetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
