{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t# Author: Alexander Staub\n",
    "\t## Last changed: 2025.06.29\n",
    "\t## Purpose: the script that cuts the data into pices according to the number of workers employed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing packages\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating worker input directory: //bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/worker_inputs_chartmetric_ids/\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "\n",
    "NUM_WORKERS = 3  # The number of parallel scripts you want to run\n",
    "\n",
    "# ---  Define the path to your sample dataset ---\n",
    "# MASTER_INPUT_FILE = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/chartmetric_ids_sample.csv\"\n",
    "\n",
    "# --- CHANGE: Define the path to your full dataset ---\n",
    "MASTER_INPUT_FILE = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/Spotify/1980_2000_songs_artists/final_charted_from_spotify_1980_2000.csv\"\n",
    "\n",
    "# --- Define a new directory where the split input files will be saved ---\n",
    "WORKER_INPUT_DIR = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/worker_inputs_chartmetric_ids/\"\n",
    "\n",
    "### WHAT THIS CHANGE DOES:\n",
    "# This separates the logic. This notebook acts as the main setup script.\n",
    "# It defines a new, dedicated folder to hold the input files for each worker, keeping your project organized.\n",
    "\n",
    "print(f\"Creating worker input directory: {WORKER_INPUT_DIR}\")\n",
    "os.makedirs(WORKER_INPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading master dataset from //bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/Spotify/1980_2000_songs_artists/final_charted_from_spotify_1980_2000.csv...\n"
     ]
    }
   ],
   "source": [
    "# --- Load and Prepare the Master Dataset ---\n",
    "print(f\"Loading master dataset from {MASTER_INPUT_FILE}...\")\n",
    "master_df = pd.read_csv(MASTER_INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data ONCE before splitting\n",
    "master_df = master_df.drop_duplicates(subset='spotify_isrc', keep='first').reset_index(drop=True)\n",
    "# --- IMPORTANT: Initialize the new column for the fresh run ---\n",
    "master_df[\"chartmetric_ids\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\music_data_chartmetric\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# --- Split the DataFrame into Chunks for Each Worker ---\n",
    "id_chunks = np.array_split(master_df, NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 1 with 8019 IDs to //bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/worker_inputs_chartmetric_ids/ids_part_1.csv\n",
      "Saved chunk 2 with 8019 IDs to //bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/worker_inputs_chartmetric_ids/ids_part_2.csv\n",
      "Saved chunk 3 with 8018 IDs to //bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/worker_inputs_chartmetric_ids/ids_part_3.csv\n",
      "\n",
      "Controller script finished. You can now run the worker notebooks.\n"
     ]
    }
   ],
   "source": [
    "# --- Save Each Chunk to its Own File ---\n",
    "for i, chunk in enumerate(id_chunks):\n",
    "    part_number = i + 1\n",
    "    # --- The output path is now dynamic for each worker part. ---\n",
    "    output_path = os.path.join(WORKER_INPUT_DIR, f\"ids_part_{part_number}.csv\")\n",
    "    \n",
    "    ### WHAT THIS CHANGE DOES:\n",
    "    # It creates separate, numbered input files (e.g., ids_part_1.csv, ids_part_2.csv).\n",
    "    # Each file contains a unique and non-overlapping subset of the original IDs.\n",
    "    \n",
    "    chunk.to_csv(output_path, index=False)\n",
    "    print(f\"Saved chunk {part_number} with {len(chunk)} IDs to {output_path}\")\n",
    "\n",
    "print(\"\\nController script finished. You can now run the worker notebooks.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_data_chartmetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
