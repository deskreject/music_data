{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "author: Alexander Staub\n",
    "Date: 2024-11-07e\n",
    "Description: Script to create a random sample of 4500 albums (300 per country) for Alessio to use to test the new Spotify code \n",
    "meant to scrape spotify audio charactersitics\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install necessary packages\n",
    "\n",
    "#library loading\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json # to read json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed for reproducibility\n",
    "import random\n",
    "random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  id_release  date_year  date_month  date_day  id_country  \\\n",
      "0           1     1968996       2010         NaN       NaN          14   \n",
      "1           2     1427936       2005         NaN       NaN          14   \n",
      "2           3       68494       1994         9.0      19.0          14   \n",
      "3           4       97102       1997         NaN       NaN          14   \n",
      "4           5      116365       1989         NaN       NaN          14   \n",
      "\n",
      "  name_country          name_label      name_label_type  \\\n",
      "0      Austria             monkey.            Publisher   \n",
      "1      Austria  Non Visual Objects  Original Production   \n",
      "2      Austria  KOCH International           Production   \n",
      "3      Austria  KOCH International           Production   \n",
      "4      Austria              Amadeo  Original Production   \n",
      "\n",
      "                           mbid_release        name_release  \\\n",
      "0  716787a0-b8e0-43fd-a056-c228015c1b75     Maximum Minisex   \n",
      "1  69020faf-0f54-463b-adc7-852ebb1b693c           Bradycard   \n",
      "2  fa29306e-b128-4e19-a9ac-f0256696a688  Im Namen der Liebe   \n",
      "3  587ae7f9-7432-3aec-bc3c-4c9b0f474a63  Who Dominates Who?   \n",
      "4  89c82c55-11ad-43a0-b67a-58d882b4654f       Liagn & lochn   \n",
      "\n",
      "  name_release_g_type  id_artist_credit              artist_credit_name  \n",
      "0               Album            245346                         Minisex  \n",
      "1               Album            466986                 Heribert Friedl  \n",
      "2               Album             49605               Brunner & Brunner  \n",
      "3               Album            210470                         AccuÂ§er  \n",
      "4               Album             56009  Ostbahn Kurti & Die Chefpartie  \n"
     ]
    }
   ],
   "source": [
    "#load the data sets of relevance\n",
    "\n",
    "# Load the album data\n",
    "file_path = r\"Z:\\Data_alexander\\data\\raw_data\\musicbrainz\\sql_exports\\release_country_labels_80_10_w_artists.csv\"\n",
    "albums = pd.read_csv(file_path)\n",
    "\n",
    "print(albums.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the recordings data with the recording IDs to merge to the albums\n",
    "\n",
    "file_path_csv_1 = r\"Z:\\Data_alexander\\data\\raw_data\\musicbrainz\\sql_exports\\release_recordings_unique_80_10_part_1.csv\"\n",
    "file_path_csv_2 = r\"Z:\\Data_alexander\\data\\raw_data\\musicbrainz\\sql_exports\\release_recordings_unique_80_10_part_2.csv\"\n",
    "file_path_csv_3 = r\"Z:\\Data_alexander\\data\\raw_data\\musicbrainz\\sql_exports\\release_recordings_unique_80_10_part_3.csv\"\n",
    "file_path_csv_4 = r\"Z:\\Data_alexander\\data\\raw_data\\musicbrainz\\sql_exports\\release_recordings_unique_80_10_part_4.csv\"\n",
    "\n",
    "\n",
    "recordings_1 = pd.read_csv(file_path_csv_1)\n",
    "recordings_2 = pd.read_csv(file_path_csv_2)\n",
    "recordings_3 = pd.read_csv(file_path_csv_3)\n",
    "recordings_4 = pd.read_csv(file_path_csv_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append the 4 seperate recordings to one single recordings table\n",
    "recordings = pd.concat([recordings_1, recordings_2, recordings_3, recordings_4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a random sample of 300 albums per distinct country in the \"name_country\" column of the albums dataset\n",
    "\n",
    "# get the unique countries\n",
    "countries = albums[\"name_country\"].unique()\n",
    "\n",
    "# create a dictionary to store the samples\n",
    "samples = {}\n",
    "\n",
    "# loop over the countries and get a random sample of 300 albums per country\n",
    "for country in countries:\n",
    "    sample = albums[albums[\"name_country\"] == country].sample(300)\n",
    "    samples[country] = sample\n",
    "\n",
    "# concatenate the samples to one single dataframe\n",
    "sample = pd.concat(samples.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial clean up of the albums\n",
    "\n",
    "# remove \"other\" under the release_g_type column\n",
    "sample = sample[sample[\"name_release_g_type\"] != \"Other\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Creating the song level dataset\n",
    "\n",
    "Need to match the recording IDs to the release_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match the recordings information to the 5100 albums information based on the \"release_ID\" column\n",
    "\n",
    "# Select only the necessary columns from recordings and rename the column\n",
    "recordings_subset = recordings[['id_release', 'name_recording', 'name_artist_credit']]\n",
    "recordings_subset = recordings_subset.rename(columns={'name_artist_credit': 'name_artist_credit_recording'})\n",
    "\n",
    "# Merge the dataframes on the ID_release column\n",
    "sample_songs = pd.merge(sample, recordings_subset, on='id_release', how='left')\n",
    "\n",
    "# remove the columns \"name_artist_credit\"\n",
    "sample_songs = sample_songs.drop(columns=[\"artist_credit_name\"])\n",
    "\n",
    "# Display the first few rows of the matched dataframe\n",
    "sample_songs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning up the \"albums\" data\n",
    "\n",
    "# remove duplicates of \"release_id\" in the \"sample\" dataset\n",
    "sample = sample.drop_duplicates(subset=\"id_release\")\n",
    "\n",
    "# remove \"various artists\" and missing values under the artist_credit_name column from the sample\n",
    "sample = sample[sample[\"artist_credit_name\"] != \"Various Artists\"]\n",
    "sample = sample.dropna(subset=[\"artist_credit_name\"])\n",
    "\n",
    "# remove \"other\" and \"missing values\" under the release_g_type column\n",
    "sample = sample.dropna(subset=[\"name_release_g_type\"])\n",
    "\n",
    "#rename the dataset to sample_albums to distinguish from prior song level dataset\n",
    "sample_albums = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates from the songs dataframe\n",
    "\n",
    "# Concatenate the columns \"name_release\" and \"name_recording\" to identify duplicates\n",
    "sample_songs['combined'] = sample_songs['name_release'] + sample_songs['name_recording']\n",
    "\n",
    "# Remove duplicates based on the concatenated column\n",
    "sample_songs_clean = sample_songs.drop_duplicates(subset='combined')\n",
    "\n",
    "# Drop the temporary concatenated column\n",
    "sample_songs_clean = sample_songs.drop(columns=['combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving files\n",
    "\n",
    "#create the file paths\n",
    "\n",
    "\n",
    "# save the \"recordings\" file as a json called \"release_recordings_unique_80_10_complied.json\" in the \"sql_exports\" folder\n",
    "\n",
    "\n",
    "# save the \"albums\" file as a csv\n",
    "\n",
    "# save the \"songs\" file as a csv\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_music_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
