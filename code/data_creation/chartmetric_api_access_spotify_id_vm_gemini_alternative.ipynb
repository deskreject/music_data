{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t# Author: Alexander Staub\n",
    "\t## Last changed: 2025.02.13\n",
    "\t## Purpose: Getting the chartmetrics IDs for a list of songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing packages\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the logging of the errors\n",
    "logging.basicConfig(\n",
    "    filename='chartmetric_api.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s: %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define API host and your refresh token\n",
    "HOST = 'https://api.chartmetric.com'\n",
    "with open(\"chartmetric_refresh_token.txt\", \"r\") as f:\n",
    "    REFRESH_TOKEN = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve an access token using the refresh token\n",
    "token_response = requests.post(f'{HOST}/api/token', json={'refreshtoken': REFRESH_TOKEN})\n",
    "\n",
    "# Check if the token was retrieved successfully\n",
    "if token_response.status_code != 200:\n",
    "\n",
    "    # Log the error and raise an exception\n",
    "    logging.error(f\"Token retrieval error: {token_response.status_code}\")\n",
    "    raise Exception(f\"Error: received {token_response.status_code} from /api/token\")\n",
    "\n",
    "# Extract the access token from the response\n",
    "access_token = token_response.json()['token']\n",
    "\n",
    "# Define the headers for the API requests\n",
    "headers = {'Authorization': f'Bearer {access_token}'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the get_request\n",
    "\n",
    "Robust request logic that:\n",
    "- backs off for a max of 26 hours in retries\n",
    "- logs all erros it encounters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Robust get_request Function ---\n",
    "def get_request(endpoint, params=None, max_retries=5):\n",
    "    backoff = 1  # initial backoff in seconds (used if header data is missing)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(f\"{HOST}{endpoint}\", headers=headers, params=params)\n",
    "        except Exception as ex:\n",
    "            logging.error(f\"Network error on attempt {attempt+1} for {endpoint}: {ex}\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "            continue\n",
    "\n",
    "# Log the response status code and rate limit headers\n",
    "        logging.info(f\"Request to {endpoint} returned {response.status_code}. RateLimit headers: {response.headers}\")\n",
    "\n",
    "# Check if the response status code is 200\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "\n",
    "# Handle different types of errors\n",
    "# 401: Token may have expired; refresh it\n",
    "        elif response.status_code == 401:\n",
    "            # Token may have expired; refresh it\n",
    "            logging.warning(f\"401 error for {endpoint}. Refreshing token.\")\n",
    "            token_response = requests.post(f'{HOST}/api/token', json={'refreshtoken': REFRESH_TOKEN})\n",
    "            if token_response.status_code != 200:\n",
    "                logging.error(f\"Token refresh failed: {token_response.status_code}\")\n",
    "                raise Exception(f\"Token refresh failed with status {token_response.status_code}\")\n",
    "            new_token = token_response.json()['token']\n",
    "            headers['Authorization'] = f'Bearer {new_token}'\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "\n",
    "# 429: Rate limit exceeded; wait and retry\n",
    "        elif response.status_code == 429:\n",
    "            # Rate limit exceeded.\n",
    "            reset_timestamp = response.headers.get(\"X-RateLimit-Reset\")\n",
    "            if reset_timestamp:\n",
    "                # Wait until the time provided by the API\n",
    "                sleep_time = int(reset_timestamp) - int(time.time())\n",
    "                if sleep_time < 0:\n",
    "                    sleep_time = backoff\n",
    "            else:\n",
    "                # No wait time provided by the API; compute one that totals 26 hours over all retries.\n",
    "                total_wait_limit = 26 * 3600  # total wait time in seconds (26 hours)\n",
    "                # Sum exponential weights for remaining attempts: for i from current attempt to max_retries-1\n",
    "                remaining_weights = sum(2 ** i for i in range(attempt, max_retries))\n",
    "                # Use the weight for the current attempt to assign a fraction of the total wait.\n",
    "                sleep_time = total_wait_limit * (2 ** attempt / remaining_weights)\n",
    "            logging.warning(f\"429 error for {endpoint}. Sleeping for {sleep_time} seconds (attempt {attempt+1}/{max_retries}).\")\n",
    "            time.sleep(sleep_time)\n",
    "            backoff *= 2\n",
    "\n",
    "# 500: Server error; wait and retry\n",
    "        elif response.status_code >= 500:\n",
    "            logging.warning(f\"Server error {response.status_code} for {endpoint}. Retrying after {backoff} seconds.\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "\n",
    "        else:\n",
    "            logging.error(f\"Error {response.status_code} for {endpoint}: {response.text}\")\n",
    "            raise Exception(f\"Error: received {response.status_code} from {endpoint}\")\n",
    "\n",
    "# If the loop completes without returning, raise an exception\n",
    "    raise Exception(f\"Max retries exceeded for endpoint {endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use isrc to access song characteristics:\n",
    "- load in a subset of around 5000 of the spotify songs with spotify IDs\n",
    "- Loop over them to get the chartmetric IDs and any other further information accessible\n",
    "- use chartmetric IDs to get the information of relevance\n",
    "- setup code in a way that allows us to get information for our relevant songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Full Original Dataset ---\n",
      "Master dataframe loaded with 3,169,134 rows to process.\n"
     ]
    }
   ],
   "source": [
    "# The single source of truth for all tracks that need to be processed.\n",
    "# CHANGE: musicbrainz data\n",
    "ORIGINAL_DATA_FILE = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/Spotify/1980_2000_songs_artists/musicbrainz_spotify_combined_track_artist_final.csv\"\n",
    "\n",
    "# CHANGE: Chart data\n",
    "# ORIGINAL_DATA_FILE = \n",
    "\n",
    "# The checkpoint file where all completed results are stored.\n",
    "CHECKPOINT_FILE = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/chartmetric_track_id_checkpoints/chartmetric_ids_checkpoint.csv\"\n",
    "\n",
    "\n",
    "print(\"--- Loading Full Original Dataset ---\")\n",
    "logging.info(f\"Loading the master dataset from: {ORIGINAL_DATA_FILE}\")\n",
    "\n",
    "# Your original loading and cleaning logic\n",
    "spotify_album = pd.read_csv(ORIGINAL_DATA_FILE)\n",
    "spotify_merged = spotify_album # Assuming this is the merge logic you need\n",
    "spotify_fetch = spotify_merged.drop_duplicates(subset='spotify_isrc', keep='first').reset_index(drop=True)\n",
    "\n",
    "print(f\"Master dataframe loaded with {len(spotify_fetch):,} rows to process.\")\n",
    "logging.info(f\"Master dataframe loaded with {len(spotify_fetch):,} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to Retrieve Chartmetric ID for an ISRC ---\n",
    "def get_chartmetric_ids(isrc):\n",
    "    endpoint = f\"/api/track/isrc/{isrc}/get-ids\"\n",
    "    try:\n",
    "        response = get_request(endpoint)\n",
    "    \n",
    "    # Log the response status code and rate limit headers\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to get Chartmetric ID for ISRC {isrc}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Expecting response[\"obj\"] to be a non-empty list\n",
    "    if response.get(\"obj\") and isinstance(response[\"obj\"], list) and len(response[\"obj\"]) > 0:\n",
    "\n",
    "        # Extract the chartmetric_ids from the first element of the list\n",
    "        cm_ids = response[\"obj\"][0].get(\"chartmetric_ids\", None)\n",
    "\n",
    "        # Check if cm_ids is a non-empty list\n",
    "        if cm_ids and isinstance(cm_ids, list) and len(cm_ids) > 0:\n",
    "            try:\n",
    "                return float(cm_ids[0])\n",
    "            \n",
    "            # Log conversion errors\n",
    "            except Exception as conv_err:\n",
    "                logging.error(f\"Conversion error for ISRC {isrc}: {conv_err}\")\n",
    "                return None\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Main Processing Loop ---\n",
      "Checkpoint file found. Loading processed ISRCs from: //bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/chartmetric_track_id_checkpoints/chartmetric_ids_checkpoint.csv\n",
      "Loaded 176,999 previously completed ISRCs. They will be skipped.\n",
      "Beginning iteration over 3169134 total rows.\n",
      "Processing row 13565/3169134: ISRC = nan\n",
      "Processing row 177000/3169134: ISRC = FR6V80309811\n",
      "Processing row 177001/3169134: ISRC = FRX850500016\n",
      "Processing row 177002/3169134: ISRC = USBS50510433\n",
      "Processing row 177003/3169134: ISRC = DEE219265101\n",
      "Processing row 177004/3169134: ISRC = DEE219265107\n",
      "Processing row 177005/3169134: ISRC = DEE219265108\n",
      "Processing row 177006/3169134: ISRC = DEE219265113\n",
      "Processing row 177007/3169134: ISRC = USAR10200427\n",
      "Processing row 177008/3169134: ISRC = USA370513720\n",
      "Processing row 177009/3169134: ISRC = FRW110300749\n",
      "Processing row 177010/3169134: ISRC = FRW110300750\n",
      "Processing row 177011/3169134: ISRC = FRW110300751\n",
      "Processing row 177012/3169134: ISRC = FRW111104248\n",
      "Processing row 177013/3169134: ISRC = FRW111104249\n",
      "Processing row 177014/3169134: ISRC = FRW110300754\n",
      "Processing row 177015/3169134: ISRC = FRW111104251\n",
      "Processing row 177016/3169134: ISRC = FRW110300756\n",
      "Processing row 177017/3169134: ISRC = FRW111104254\n",
      "Processing row 177018/3169134: ISRC = FRW110300759\n",
      "Processing row 177019/3169134: ISRC = FRW110300760\n",
      "Processing row 177020/3169134: ISRC = FRW110300761\n",
      "Processing row 177021/3169134: ISRC = FRW110300762\n",
      "Processing row 177022/3169134: ISRC = FRW111104259\n"
     ]
    }
   ],
   "source": [
    "# --- Main Processing with EFFICIENT, RESUMABLE Checkpointing ---\n",
    "TIME_PER_REQUEST = 0.28 \n",
    "checkpoint_interval = 1000  # Set to 1000 as requested\n",
    "\n",
    "\n",
    "print(\"--- Initializing Main Processing Loop ---\")\n",
    "\n",
    "# A temporary list to hold new results before writing to file\n",
    "results_buffer = []\n",
    "\n",
    "try:\n",
    "    # --- STEP 2a: Load the set of already processed ISRCs for fast lookups ---\n",
    "    # This is the \"resume\" part. We check the checkpoint file to see what's already done.\n",
    "    processed_isrcs = set()\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        print(f\"Checkpoint file found. Loading processed ISRCs from: {CHECKPOINT_FILE}\")\n",
    "        logging.info(f\"Loading processed ISRCs from checkpoint: {CHECKPOINT_FILE}\")\n",
    "        try:\n",
    "            # We only need the 'spotify_isrc' column to know what's already done.\n",
    "            processed_df = pd.read_csv(CHECKPOINT_FILE, usecols=['spotify_isrc'])\n",
    "            processed_isrcs = set(processed_df['spotify_isrc'].dropna().unique())\n",
    "            print(f\"Loaded {len(processed_isrcs):,} previously completed ISRCs. They will be skipped.\")\n",
    "            logging.info(f\"Loaded {len(processed_isrcs):,} previously completed ISRCs.\")\n",
    "        except (pd.errors.EmptyDataError, KeyError, FileNotFoundError):\n",
    "            print(\"Checkpoint file is empty or invalid. A new one will be created.\")\n",
    "            logging.warning(\"Checkpoint file was found but is empty or invalid.\")\n",
    "            # If the file is broken, we start fresh and create the header.\n",
    "            pd.DataFrame(columns=['spotify_isrc', 'chartmetric_ids']).to_csv(CHECKPOINT_FILE, index=False)\n",
    "    else:\n",
    "        print(\"No checkpoint file found. Creating a new one with headers.\")\n",
    "        logging.info(\"No checkpoint file found. Creating a new one.\")\n",
    "        # If the file doesn't exist, create it with the necessary columns.\n",
    "        pd.DataFrame(columns=['spotify_isrc', 'chartmetric_ids']).to_csv(CHECKPOINT_FILE, index=False)\n",
    "\n",
    "    \n",
    "    \n",
    "    # --- STEP 2: Iterate through the main dataframe ---\n",
    "    total_rows = len(spotify_fetch)\n",
    "    print(f\"Beginning iteration over {total_rows} total rows.\")\n",
    "    \n",
    "    for idx, row in spotify_fetch.iterrows():\n",
    "        isrc = row['spotify_isrc']\n",
    "        \n",
    "        # --- EFFICIENT RESUME LOGIC ---\n",
    "        # If we have already processed this ISRC in a previous run, skip it immediately.\n",
    "        if isrc in processed_isrcs:\n",
    "            continue\n",
    "\n",
    "        loop_start_time = time.time()\n",
    "        print(f\"Processing row {idx}/{total_rows}: ISRC = {isrc}\")\n",
    "\n",
    "        try:\n",
    "            chartmetric_id = get_chartmetric_ids(isrc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing ISRC {isrc} at row {idx}: {e}\")\n",
    "            logging.error(f\"Error processing ISRC {isrc} at row {idx}: {e}\")\n",
    "            chartmetric_id = None\n",
    "        \n",
    "        # Add the new result to our temporary buffer instead of the giant dataframe\n",
    "        results_buffer.append({\n",
    "            'spotify_isrc': isrc, \n",
    "            'chartmetric_ids': chartmetric_id\n",
    "        })\n",
    "        \n",
    "        # Add to our in-memory set to avoid re-processing in the *same* run if there are duplicates\n",
    "        processed_isrcs.add(isrc)\n",
    "        \n",
    "        logging.info(f\"Buffered ISRC {isrc}: Chartmetric ID = {chartmetric_id}\")\n",
    "\n",
    "        # --- DYNAMIC SLEEP LOGIC ---\n",
    "        elapsed_time = time.time() - loop_start_time\n",
    "        sleep_duration = TIME_PER_REQUEST - elapsed_time\n",
    "        if sleep_duration > 0:\n",
    "            time.sleep(sleep_duration)\n",
    "        \n",
    "        # --- FAST APPEND-BASED CHECKPOINT ---\n",
    "        # When the buffer is full, append it to the checkpoint file. This is very fast.\n",
    "        if len(results_buffer) >= checkpoint_interval:\n",
    "            print(f\"--- Checkpoint interval reached. Appending {len(results_buffer)} new results... ---\")\n",
    "            checkpoint_df = pd.DataFrame(results_buffer)\n",
    "            # Use mode='a' (append) and header=False to efficiently add to the existing file\n",
    "            checkpoint_df.to_csv(CHECKPOINT_FILE, mode='a', header=False, index=False)\n",
    "            results_buffer = [] # Reset the buffer for the next batch\n",
    "            print(f\"--- Batch appended to checkpoint file. ---\")\n",
    "            logging.info(f\"Appended {len(checkpoint_df)} rows to checkpoint.\")\n",
    "\n",
    "finally:\n",
    "    # --- FINAL SAVE OF REMAINING RESULTS ---\n",
    "    # This ensures that even if the loop is interrupted, the last batch of results is saved.\n",
    "    if results_buffer:\n",
    "        print(f\"\\nLoop finished or was interrupted. Appending {len(results_buffer)} final results...\")\n",
    "        final_df = pd.DataFrame(results_buffer)\n",
    "        final_df.to_csv(CHECKPOINT_FILE, mode='a', header=False, index=False)\n",
    "        print(f\"Final batch appended to: {CHECKPOINT_FILE}\")\n",
    "        logging.info(f\"Appended {len(final_df)} final rows to checkpoint.\")\n",
    "    \n",
    "    print(\"\\nProcessing complete.\")\n",
    "    logging.info(\"All processing is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge in the chartmetric ids into the original dataframe\n",
    "chartmetric_ids_df = pd.read_csv(\"//bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/chartmetric_track_id_checkpoints/chartmetric_ids_checkpoint.csv\")\n",
    "\n",
    "spotify_fetch = pd.merge(spotify_fetch, chartmetric_ids_df, on='spotify_isrc', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the object type into float \n",
    "spotify_fetch[\"chartmetric_ids\"] = pd.to_numeric(spotify_fetch[\"chartmetric_ids\"], errors='coerce')\n",
    "\n",
    "#transform into integer\n",
    "spotify_fetch[\"chartmetric_ids\"] = spotify_fetch[\"chartmetric_ids\"].astype(\"Int64\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retain only the rows with unique chartmetric_ids\n",
    "spotify_fetch_unique = spotify_fetch.drop_duplicates(subset=\"chartmetric_ids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the files needs to take into account the version that I have already saved in the past as I am not able to run the code remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file as: //bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/chartmetric_ids_sample.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Define the final file path\n",
    "# NEED TO CHECK:  the suffix\n",
    "filepath = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/\"\n",
    "\n",
    "#give the file a name\n",
    "file_name = \"chartmetric_ids_mb_matched.csv\"\n",
    "\n",
    "#paste filepath and file_name together and call the variable final_filepath\n",
    "\n",
    "final_filepath = os.path.join(filepath, file_name)\n",
    "\n",
    "# Save the dataframe to the final_filepath\n",
    "spotify_fetch_unique.to_csv(final_filepath, index=False)\n",
    "print(f\"Saved file as: {final_filepath}\")\n",
    "\n",
    "# Log the final message in the log file\n",
    "logging.info(\"Completed processing all ISRC codes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_data_chartmetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
