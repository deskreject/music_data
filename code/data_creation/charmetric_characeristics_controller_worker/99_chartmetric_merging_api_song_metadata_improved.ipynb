{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t# Author: Alexander Staub\n",
    "\t## Last changed: 2025.08.26\n",
    "\t## Purpose: Using the chartmetric IDs to get song level metadata post spotify data collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing packages\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from typing import Dict, Any, List, Optional\n",
    "import gc  # For garbage collection\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Cell 1: Configuration ---\n",
    "\n",
    "# Set your paths here - these become notebook-wide variables you can inspect\n",
    "METADATA_OUTPUT_DIR =  \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_chars/\"\n",
    "OUTPUT_CSV_PATH = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_chars/complete_charmetric_chars.csv\"\n",
    "OUTPUT_PARQUET_PATH = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_chars/complete_charmetric_chars.parquet\"\n",
    "\n",
    "# Configuration parameters - easy to modify and experiment with\n",
    "BATCH_SIZE = 100  # Process this many files at a time\n",
    "WRITE_CHUNK_SIZE = 20000  # Write to CSV after processing this many records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction function defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Define the extraction function\n",
    "def extract_song_info_dict(search_output: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Extract song info as a dictionary (more memory efficient than DataFrame)\"\"\"\n",
    "    obj = search_output.get('obj', {})\n",
    "    \n",
    "    # Artist information\n",
    "    artist_id = artist_name = artist_label = artist_booking_agent = artist_general_manager = None\n",
    "    if obj.get('artists') and len(obj['artists']) > 0:\n",
    "        artist = obj['artists'][0]\n",
    "        artist_id = artist.get('id')\n",
    "        artist_name = artist.get('name')\n",
    "        artist_label = artist.get('label')\n",
    "        artist_booking_agent = artist.get('booking_agent')\n",
    "        artist_general_manager = artist.get('general_manager')\n",
    "    \n",
    "    # Album information (earliest release)\n",
    "    album_id = album_name = album_release_date = album_label = None\n",
    "    if obj.get('albums') and len(obj['albums']) > 0:\n",
    "        def parse_date(album):\n",
    "            try:\n",
    "                return datetime.strptime(album.get('release_date', ''), '%Y-%m-%d')\n",
    "            except:\n",
    "                return datetime.max\n",
    "        \n",
    "        sorted_albums = sorted(obj['albums'], key=parse_date)\n",
    "        earliest_album = sorted_albums[0]\n",
    "        album_id = earliest_album.get('id')\n",
    "        album_name = earliest_album.get('name')\n",
    "        album_release_date = earliest_album.get('release_date')\n",
    "        album_label = earliest_album.get('label')\n",
    "    \n",
    "    # Delimiter for multiple values\n",
    "    delimiter = ','\n",
    "    \n",
    "    # Moods\n",
    "    moods = None\n",
    "    if obj.get('moods') and len(obj['moods']) > 0:\n",
    "        moods = delimiter.join([m.get('name', '') for m in obj['moods'] if m.get('name')])\n",
    "    \n",
    "    # Activities\n",
    "    activities = None\n",
    "    if obj.get('activities') and len(obj['activities']) > 0:\n",
    "        activities = delimiter.join([a.get('name', '') for a in obj['activities'] if a.get('name')])\n",
    "    \n",
    "    # Songwriters\n",
    "    songwriters = None\n",
    "    if obj.get('songwriters') and len(obj['songwriters']) > 0:\n",
    "        songwriters = delimiter.join(obj['songwriters'])\n",
    "    \n",
    "    return {\n",
    "        'chartmetric_ids': obj.get('id'),\n",
    "        'cm_track_title': obj.get('name'),\n",
    "        'cm_composer_name': obj.get('composer_name'),\n",
    "        'cm_artist_id': artist_id,\n",
    "        'cm_artist_credit': artist_name,\n",
    "        'cm_artist_label': artist_label,\n",
    "        'Artist_booking_agent': artist_booking_agent,\n",
    "        'Artist_general_manager': artist_general_manager,\n",
    "        'cm_release_id': album_id,\n",
    "        'cm_name_release': album_name,\n",
    "        'cm_release_date': album_release_date,\n",
    "        'cm_albums_label': album_label,\n",
    "        'cm_genres': obj.get('tags'),\n",
    "        'cm_moods': moods,\n",
    "        'cm_activities': activities,\n",
    "        'cm_songwriters': songwriters,\n",
    "        'cm_songwriterIds': None,\n",
    "        'cm_tempo': obj.get('tempo'),\n",
    "        'cm_duration_ms': obj.get('duration_ms')\n",
    "    }\n",
    "\n",
    "print(\"Extraction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2904 batch files\n",
      "Total estimated size: ~43560 MB\n",
      "\n",
      "First 5 files:\n",
      "  - batch_00001.json\n",
      "  - batch_00002.json\n",
      "  - batch_00003.json\n",
      "  - batch_00004.json\n",
      "  - batch_00005.json\n",
      "\n",
      "First file contains 1000 records\n",
      "Sample record structure:\n",
      "Extracted fields: ['chartmetric_ids', 'cm_track_title', 'cm_composer_name', 'cm_artist_id', 'cm_artist_credit', 'cm_artist_label', 'Artist_booking_agent', 'Artist_general_manager', 'cm_release_id', 'cm_name_release', 'cm_release_date', 'cm_albums_label', 'cm_genres', 'cm_moods', 'cm_activities', 'cm_songwriters', 'cm_songwriterIds', 'cm_tempo', 'cm_duration_ms']\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Explore your data structure \n",
    "# Find and examine the files\n",
    "search_pattern = os.path.join(METADATA_OUTPUT_DIR, \"part_*\", \"responses\", \"batch_*.json\")\n",
    "all_batch_files = sorted(glob.glob(search_pattern))\n",
    "\n",
    "print(f\"Found {len(all_batch_files)} batch files\")\n",
    "print(f\"Total estimated size: ~{len(all_batch_files) * 15} MB\")\n",
    "print(\"\\nFirst 5 files:\")\n",
    "for file in all_batch_files[:5]:\n",
    "    print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "# Optional: Peek at one file to understand structure\n",
    "if all_batch_files:\n",
    "    with open(all_batch_files[0], 'r') as f:\n",
    "        sample_data = json.load(f)\n",
    "    print(f\"\\nFirst file contains {len(sample_data)} records\")\n",
    "    print(\"Sample record structure:\")\n",
    "    if sample_data:\n",
    "        # Test the extraction on one record\n",
    "        sample_extracted = extract_song_info_dict(sample_data[0])\n",
    "        print(f\"Extracted fields: {list(sample_extracted.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Main processing function (for CSV output)\n",
    "def process_batch_files_incrementally():\n",
    "    \"\"\"Process JSON files in batches and write to CSV incrementally\"\"\"\n",
    "    \n",
    "    # Use the global variables defined in Cell 1\n",
    "    global all_batch_files, total_records_processed, failed_files, failed_records\n",
    "    \n",
    "    total_files = len(all_batch_files)\n",
    "    \n",
    "    print(f\"Starting processing of {total_files} files\")\n",
    "    print(f\"Processing in batches of {BATCH_SIZE} files\")\n",
    "    print(f\"Writing to CSV every {WRITE_CHUNK_SIZE} records\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Initialize CSV writer\n",
    "    csv_file = open(OUTPUT_CSV_PATH, 'w', newline='', encoding='utf-8')\n",
    "    csv_writer = None\n",
    "    \n",
    "    # Track progress - these become inspectable variables\n",
    "    total_records_processed = 0\n",
    "    records_buffer = []\n",
    "    failed_files = []\n",
    "    failed_records = []\n",
    "    \n",
    "    try:\n",
    "        # Process files in batches\n",
    "        for batch_start in range(0, total_files, BATCH_SIZE):\n",
    "            batch_end = min(batch_start + BATCH_SIZE, total_files)\n",
    "            batch_files = all_batch_files[batch_start:batch_end]\n",
    "            \n",
    "            print(f\"\\nðŸ“¦ Processing batch {batch_start//BATCH_SIZE + 1}/{(total_files + BATCH_SIZE - 1)//BATCH_SIZE}\")\n",
    "            print(f\"   Files {batch_start + 1} to {batch_end} of {total_files}\")\n",
    "            \n",
    "            # Process each file in the batch\n",
    "            for file_idx, file_path in enumerate(batch_files, start=batch_start):\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        batch_data = json.load(f)\n",
    "                    \n",
    "                    # Process each record\n",
    "                    for record_idx, record in enumerate(batch_data):\n",
    "                        try:\n",
    "                            extracted_data = extract_song_info_dict(record)\n",
    "                            records_buffer.append(extracted_data)\n",
    "                            \n",
    "                            # Initialize CSV writer with headers from first record\n",
    "                            if csv_writer is None and extracted_data:\n",
    "                                csv_writer = csv.DictWriter(csv_file, fieldnames=list(extracted_data.keys()))\n",
    "                                csv_writer.writeheader()\n",
    "                            \n",
    "                            # Write to CSV when buffer is full\n",
    "                            if len(records_buffer) >= WRITE_CHUNK_SIZE:\n",
    "                                csv_writer.writerows(records_buffer)\n",
    "                                csv_file.flush()  # Ensure data is written to disk\n",
    "                                total_records_processed += len(records_buffer)\n",
    "                                print(f\" Written {total_records_processed:,} records to CSV\")\n",
    "                                records_buffer = []\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            failed_records.append({\n",
    "                                'file': file_path,\n",
    "                                'record_index': record_idx,\n",
    "                                'error': str(e)\n",
    "                            })\n",
    "                            if len(failed_records) <= 10:\n",
    "                                print(f\" Failed to process record {record_idx}: {e}\")\n",
    "                    \n",
    "                    # Clear the batch_data from memory\n",
    "                    del batch_data\n",
    "                    \n",
    "                    # Progress indicator\n",
    "                    if (file_idx + 1) % 10 == 0:\n",
    "                        print(f\" Processed {file_idx + 1}/{total_files} files\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    failed_files.append({'file': file_path, 'error': str(e)})\n",
    "                    print(f\" Error reading {os.path.basename(file_path)}: {e}\")\n",
    "            \n",
    "            # Force garbage collection after each batch\n",
    "            gc.collect()\n",
    "        \n",
    "        # Write any remaining records\n",
    "        if records_buffer and csv_writer:\n",
    "            csv_writer.writerows(records_buffer)\n",
    "            total_records_processed += len(records_buffer)\n",
    "            print(f\"\\nâœ“ Written final {len(records_buffer)} records\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\" PROCESSING COMPLETE\")\n",
    "        print(f\"Total records processed: {total_records_processed:,}\")\n",
    "        print(f\"Failed files: {len(failed_files)}\")\n",
    "        print(f\"Failed records: {len(failed_records)}\")\n",
    "        print(f\"Output saved to: {OUTPUT_CSV_PATH}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Save error logs if any\n",
    "        if failed_files or failed_records:\n",
    "            error_log_path = OUTPUT_CSV_PATH.replace('.csv', '_errors.json')\n",
    "            with open(error_log_path, 'w') as f:\n",
    "                json.dump({\n",
    "                    'failed_files': failed_files,\n",
    "                    'failed_records': failed_records[:1000]  # Limit to first 1000 errors\n",
    "                }, f, indent=2)\n",
    "            print(f\"Error log saved to: {error_log_path}\")\n",
    "            \n",
    "    finally:\n",
    "        csv_file.close()\n",
    "    \n",
    "    return total_records_processed, failed_files, failed_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10 files for testing...\n",
      "Created test DataFrame with 9411 records\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "chartmetric_ids",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cm_track_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cm_composer_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cm_artist_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cm_artist_credit",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cm_artist_label",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Artist_booking_agent",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Artist_general_manager",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cm_release_id",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cm_name_release",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cm_release_date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cm_albums_label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cm_genres",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cm_moods",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cm_activities",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cm_songwriters",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cm_songwriterIds",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cm_tempo",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cm_duration_ms",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "277e13cc-195e-4999-8e08-3419b16ab991",
       "rows": [
        [
         "0",
         "12486264",
         "Uptown Girl - Radio Edit",
         "Billy Joel",
         "208701",
         "Westlife",
         null,
         null,
         null,
         "1170038.0",
         "Coast To Coast",
         "2000-01-01",
         "RCA Records Label",
         "soft pop,pop,teen pop,rock,modern rock,alternative rock,dance,electropop",
         "affectionate,nostalgic,sarcastic",
         "daydreaming",
         "Billy Joel",
         null,
         "129.032",
         "187666.0"
        ],
        [
         "1",
         "12486289",
         "Let's Dance - Radio Edit",
         "Sean Conlon, Martin Harrington, Ash Howes, Richard Stannard, Richard Abidin Breen, Richard Abidin Breen, Richard Stannard",
         "208971",
         "Five",
         null,
         null,
         null,
         "703326.0",
         "Let's Dance",
         "2001-01-16",
         "RCA Camden",
         "pop,teen pop,dance,rock,electronic,soft pop",
         "energetic,entertaining,fun,funky",
         "dancy,partying",
         "Sean Conlon,Martin Harrington,Ash Howes,Richard Stannard,Richard Abidin Breen,Richard Abidin Breen,Richard Stannard",
         null,
         "118.024",
         "219853.0"
        ],
        [
         "2",
         "16790822",
         "My Boyfriend",
         "J. Ballard",
         "222162",
         "Bubbles",
         null,
         null,
         null,
         "1585098.0",
         "Rock the World",
         "2002-01-01",
         "PCA Music",
         "dance",
         null,
         null,
         "J. Ballard",
         null,
         "106.024",
         "181840.0"
        ],
        [
         "3",
         "10926819",
         "1 2 3",
         "Ramon Garriga, Frank Madero, Puerta",
         "138307",
         "El Simbolo",
         null,
         null,
         null,
         "210241.0",
         "Ã‰xitos",
         "2005-01-01",
         "Hit Designers",
         "latin pop,latin",
         "fun",
         "dancy,bonding,daydreaming",
         "Ramon Garriga,Frank Madero,Puerta",
         null,
         "131.023",
         "219933.0"
        ],
        [
         "4",
         "11060010",
         "Tele-Romeo",
         "Peter Jules Gillis, Miguel Jose Eric Wiels, Alain Robert Anna Vande Putte",
         "109874",
         "K3",
         null,
         null,
         null,
         "1163710.0",
         "Tele Romeo",
         "2001-01-01",
         "Studio 100",
         "dutch pop,dutch children's music",
         "catchy,cheerful,entertaining,fun,childlike,energetic",
         "summer,roadtrip",
         "Peter Jules Gillis,Miguel Jose Eric Wiels,Alain Robert Anna Vande Putte",
         null,
         "130.011",
         "199575.0"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chartmetric_ids</th>\n",
       "      <th>cm_track_title</th>\n",
       "      <th>cm_composer_name</th>\n",
       "      <th>cm_artist_id</th>\n",
       "      <th>cm_artist_credit</th>\n",
       "      <th>cm_artist_label</th>\n",
       "      <th>Artist_booking_agent</th>\n",
       "      <th>Artist_general_manager</th>\n",
       "      <th>cm_release_id</th>\n",
       "      <th>cm_name_release</th>\n",
       "      <th>cm_release_date</th>\n",
       "      <th>cm_albums_label</th>\n",
       "      <th>cm_genres</th>\n",
       "      <th>cm_moods</th>\n",
       "      <th>cm_activities</th>\n",
       "      <th>cm_songwriters</th>\n",
       "      <th>cm_songwriterIds</th>\n",
       "      <th>cm_tempo</th>\n",
       "      <th>cm_duration_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12486264</td>\n",
       "      <td>Uptown Girl - Radio Edit</td>\n",
       "      <td>Billy Joel</td>\n",
       "      <td>208701</td>\n",
       "      <td>Westlife</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1170038.0</td>\n",
       "      <td>Coast To Coast</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>RCA Records Label</td>\n",
       "      <td>soft pop,pop,teen pop,rock,modern rock,alterna...</td>\n",
       "      <td>affectionate,nostalgic,sarcastic</td>\n",
       "      <td>daydreaming</td>\n",
       "      <td>Billy Joel</td>\n",
       "      <td>None</td>\n",
       "      <td>129.032</td>\n",
       "      <td>187666.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12486289</td>\n",
       "      <td>Let's Dance - Radio Edit</td>\n",
       "      <td>Sean Conlon, Martin Harrington, Ash Howes, Ric...</td>\n",
       "      <td>208971</td>\n",
       "      <td>Five</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>703326.0</td>\n",
       "      <td>Let's Dance</td>\n",
       "      <td>2001-01-16</td>\n",
       "      <td>RCA Camden</td>\n",
       "      <td>pop,teen pop,dance,rock,electronic,soft pop</td>\n",
       "      <td>energetic,entertaining,fun,funky</td>\n",
       "      <td>dancy,partying</td>\n",
       "      <td>Sean Conlon,Martin Harrington,Ash Howes,Richar...</td>\n",
       "      <td>None</td>\n",
       "      <td>118.024</td>\n",
       "      <td>219853.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16790822</td>\n",
       "      <td>My Boyfriend</td>\n",
       "      <td>J. Ballard</td>\n",
       "      <td>222162</td>\n",
       "      <td>Bubbles</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1585098.0</td>\n",
       "      <td>Rock the World</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>PCA Music</td>\n",
       "      <td>dance</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>J. Ballard</td>\n",
       "      <td>None</td>\n",
       "      <td>106.024</td>\n",
       "      <td>181840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10926819</td>\n",
       "      <td>1 2 3</td>\n",
       "      <td>Ramon Garriga, Frank Madero, Puerta</td>\n",
       "      <td>138307</td>\n",
       "      <td>El Simbolo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>210241.0</td>\n",
       "      <td>Ã‰xitos</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>Hit Designers</td>\n",
       "      <td>latin pop,latin</td>\n",
       "      <td>fun</td>\n",
       "      <td>dancy,bonding,daydreaming</td>\n",
       "      <td>Ramon Garriga,Frank Madero,Puerta</td>\n",
       "      <td>None</td>\n",
       "      <td>131.023</td>\n",
       "      <td>219933.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11060010</td>\n",
       "      <td>Tele-Romeo</td>\n",
       "      <td>Peter Jules Gillis, Miguel Jose Eric Wiels, Al...</td>\n",
       "      <td>109874</td>\n",
       "      <td>K3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1163710.0</td>\n",
       "      <td>Tele Romeo</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>Studio 100</td>\n",
       "      <td>dutch pop,dutch children's music</td>\n",
       "      <td>catchy,cheerful,entertaining,fun,childlike,ene...</td>\n",
       "      <td>summer,roadtrip</td>\n",
       "      <td>Peter Jules Gillis,Miguel Jose Eric Wiels,Alai...</td>\n",
       "      <td>None</td>\n",
       "      <td>130.011</td>\n",
       "      <td>199575.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chartmetric_ids            cm_track_title  \\\n",
       "0         12486264  Uptown Girl - Radio Edit   \n",
       "1         12486289  Let's Dance - Radio Edit   \n",
       "2         16790822              My Boyfriend   \n",
       "3         10926819                     1 2 3   \n",
       "4         11060010                Tele-Romeo   \n",
       "\n",
       "                                    cm_composer_name  cm_artist_id  \\\n",
       "0                                         Billy Joel        208701   \n",
       "1  Sean Conlon, Martin Harrington, Ash Howes, Ric...        208971   \n",
       "2                                         J. Ballard        222162   \n",
       "3                Ramon Garriga, Frank Madero, Puerta        138307   \n",
       "4  Peter Jules Gillis, Miguel Jose Eric Wiels, Al...        109874   \n",
       "\n",
       "  cm_artist_credit cm_artist_label Artist_booking_agent  \\\n",
       "0         Westlife            None                 None   \n",
       "1             Five            None                 None   \n",
       "2          Bubbles            None                 None   \n",
       "3       El Simbolo            None                 None   \n",
       "4               K3            None                 None   \n",
       "\n",
       "  Artist_general_manager  cm_release_id cm_name_release cm_release_date  \\\n",
       "0                   None      1170038.0  Coast To Coast      2000-01-01   \n",
       "1                   None       703326.0     Let's Dance      2001-01-16   \n",
       "2                   None      1585098.0  Rock the World      2002-01-01   \n",
       "3                   None       210241.0          Ã‰xitos      2005-01-01   \n",
       "4                   None      1163710.0      Tele Romeo      2001-01-01   \n",
       "\n",
       "     cm_albums_label                                          cm_genres  \\\n",
       "0  RCA Records Label  soft pop,pop,teen pop,rock,modern rock,alterna...   \n",
       "1         RCA Camden        pop,teen pop,dance,rock,electronic,soft pop   \n",
       "2          PCA Music                                              dance   \n",
       "3      Hit Designers                                    latin pop,latin   \n",
       "4         Studio 100                   dutch pop,dutch children's music   \n",
       "\n",
       "                                            cm_moods  \\\n",
       "0                   affectionate,nostalgic,sarcastic   \n",
       "1                   energetic,entertaining,fun,funky   \n",
       "2                                               None   \n",
       "3                                                fun   \n",
       "4  catchy,cheerful,entertaining,fun,childlike,ene...   \n",
       "\n",
       "               cm_activities  \\\n",
       "0                daydreaming   \n",
       "1             dancy,partying   \n",
       "2                       None   \n",
       "3  dancy,bonding,daydreaming   \n",
       "4            summer,roadtrip   \n",
       "\n",
       "                                      cm_songwriters cm_songwriterIds  \\\n",
       "0                                         Billy Joel             None   \n",
       "1  Sean Conlon,Martin Harrington,Ash Howes,Richar...             None   \n",
       "2                                         J. Ballard             None   \n",
       "3                  Ramon Garriga,Frank Madero,Puerta             None   \n",
       "4  Peter Jules Gillis,Miguel Jose Eric Wiels,Alai...             None   \n",
       "\n",
       "   cm_tempo  cm_duration_ms  \n",
       "0   129.032        187666.0  \n",
       "1   118.024        219853.0  \n",
       "2   106.024        181840.0  \n",
       "3   131.023        219933.0  \n",
       "4   130.011        199575.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5: Alternative - Process smaller batch for testing\n",
    "# Useful for testing before running the full processing\n",
    "def process_test_batch(num_files=10):\n",
    "    \"\"\"Process just a few files for testing\"\"\"\n",
    "    test_files = all_batch_files[:num_files]\n",
    "    test_records = []\n",
    "    \n",
    "    print(f\"Processing {num_files} files for testing...\")\n",
    "    \n",
    "    for file_path in test_files:\n",
    "        with open(file_path, 'r') as f:\n",
    "            batch_data = json.load(f)\n",
    "        \n",
    "        for record in batch_data:\n",
    "            test_records.append(extract_song_info_dict(record))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_test = pd.DataFrame(test_records)\n",
    "    print(f\"Created test DataFrame with {len(df_test)} records\")\n",
    "    \n",
    "    return df_test\n",
    "\n",
    "# Uncomment to run a test batch first\n",
    "df_test = process_test_batch(10)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing of 2904 files\n",
      "Processing in batches of 100 files\n",
      "Writing to CSV every 20000 records\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“¦ Processing batch 1/30\n",
      "   Files 1 to 100 of 2904\n",
      " Processed 10/2904 files\n",
      " Processed 20/2904 files\n",
      " Written 20,000 records to CSV\n",
      " Processed 30/2904 files\n",
      " Processed 40/2904 files\n",
      " Written 40,000 records to CSV\n",
      " Processed 50/2904 files\n",
      " Failed to process record 977: 'NoneType' object has no attribute 'get'\n",
      " Processed 60/2904 files\n",
      " Written 60,000 records to CSV\n",
      " Processed 70/2904 files\n",
      " Processed 80/2904 files\n",
      " Written 80,000 records to CSV\n",
      " Processed 90/2904 files\n",
      " Processed 100/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 2/30\n",
      "   Files 101 to 200 of 2904\n",
      " Written 100,000 records to CSV\n",
      " Failed to process record 663: 'NoneType' object has no attribute 'get'\n",
      " Failed to process record 681: 'NoneType' object has no attribute 'get'\n",
      " Processed 110/2904 files\n",
      " Processed 120/2904 files\n",
      " Written 120,000 records to CSV\n",
      " Processed 130/2904 files\n",
      " Processed 140/2904 files\n",
      " Written 140,000 records to CSV\n",
      " Failed to process record 229: 'NoneType' object has no attribute 'get'\n",
      " Failed to process record 460: 'NoneType' object has no attribute 'get'\n",
      " Failed to process record 463: 'NoneType' object has no attribute 'get'\n",
      " Processed 150/2904 files\n",
      " Processed 160/2904 files\n",
      " Written 160,000 records to CSV\n",
      " Processed 170/2904 files\n",
      " Processed 180/2904 files\n",
      " Written 180,000 records to CSV\n",
      " Processed 190/2904 files\n",
      " Processed 200/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 3/30\n",
      "   Files 201 to 300 of 2904\n",
      " Written 200,000 records to CSV\n",
      " Processed 210/2904 files\n",
      " Processed 220/2904 files\n",
      " Written 220,000 records to CSV\n",
      " Processed 230/2904 files\n",
      " Processed 240/2904 files\n",
      " Written 240,000 records to CSV\n",
      " Processed 250/2904 files\n",
      " Processed 260/2904 files\n",
      " Written 260,000 records to CSV\n",
      " Processed 270/2904 files\n",
      " Processed 280/2904 files\n",
      " Written 280,000 records to CSV\n",
      " Processed 290/2904 files\n",
      " Processed 300/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 4/30\n",
      "   Files 301 to 400 of 2904\n",
      " Written 300,000 records to CSV\n",
      " Processed 310/2904 files\n",
      " Processed 320/2904 files\n",
      " Written 320,000 records to CSV\n",
      " Processed 330/2904 files\n",
      " Processed 340/2904 files\n",
      " Written 340,000 records to CSV\n",
      " Processed 350/2904 files\n",
      " Processed 360/2904 files\n",
      " Written 360,000 records to CSV\n",
      " Processed 370/2904 files\n",
      " Processed 380/2904 files\n",
      " Written 380,000 records to CSV\n",
      " Processed 390/2904 files\n",
      " Processed 400/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 5/30\n",
      "   Files 401 to 500 of 2904\n",
      " Written 400,000 records to CSV\n",
      " Processed 410/2904 files\n",
      " Processed 420/2904 files\n",
      " Written 420,000 records to CSV\n",
      " Processed 430/2904 files\n",
      " Processed 440/2904 files\n",
      " Written 440,000 records to CSV\n",
      " Processed 450/2904 files\n",
      " Processed 460/2904 files\n",
      " Written 460,000 records to CSV\n",
      " Processed 470/2904 files\n",
      " Processed 480/2904 files\n",
      " Written 480,000 records to CSV\n",
      " Processed 490/2904 files\n",
      " Processed 500/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 6/30\n",
      "   Files 501 to 600 of 2904\n",
      " Written 500,000 records to CSV\n",
      " Processed 510/2904 files\n",
      " Processed 520/2904 files\n",
      " Written 520,000 records to CSV\n",
      " Processed 530/2904 files\n",
      " Processed 540/2904 files\n",
      " Written 540,000 records to CSV\n",
      " Processed 550/2904 files\n",
      " Processed 560/2904 files\n",
      " Written 560,000 records to CSV\n",
      " Processed 570/2904 files\n",
      " Processed 580/2904 files\n",
      " Written 580,000 records to CSV\n",
      " Processed 590/2904 files\n",
      " Processed 600/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 7/30\n",
      "   Files 601 to 700 of 2904\n",
      " Written 600,000 records to CSV\n",
      " Processed 610/2904 files\n",
      " Processed 620/2904 files\n",
      " Written 620,000 records to CSV\n",
      " Processed 630/2904 files\n",
      " Processed 640/2904 files\n",
      " Written 640,000 records to CSV\n",
      " Processed 650/2904 files\n",
      " Processed 660/2904 files\n",
      " Written 660,000 records to CSV\n",
      " Processed 670/2904 files\n",
      " Processed 680/2904 files\n",
      " Written 680,000 records to CSV\n",
      " Processed 690/2904 files\n",
      " Processed 700/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 8/30\n",
      "   Files 701 to 800 of 2904\n",
      " Written 700,000 records to CSV\n",
      " Processed 710/2904 files\n",
      " Processed 720/2904 files\n",
      " Written 720,000 records to CSV\n",
      " Processed 730/2904 files\n",
      " Processed 740/2904 files\n",
      " Written 740,000 records to CSV\n",
      " Processed 750/2904 files\n",
      " Processed 760/2904 files\n",
      " Written 760,000 records to CSV\n",
      " Processed 770/2904 files\n",
      " Processed 780/2904 files\n",
      " Written 780,000 records to CSV\n",
      " Processed 790/2904 files\n",
      " Processed 800/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 9/30\n",
      "   Files 801 to 900 of 2904\n",
      " Written 800,000 records to CSV\n",
      " Processed 810/2904 files\n",
      " Processed 820/2904 files\n",
      " Written 820,000 records to CSV\n",
      " Processed 830/2904 files\n",
      " Processed 840/2904 files\n",
      " Written 840,000 records to CSV\n",
      " Processed 850/2904 files\n",
      " Processed 860/2904 files\n",
      " Written 860,000 records to CSV\n",
      " Processed 870/2904 files\n",
      " Processed 880/2904 files\n",
      " Written 880,000 records to CSV\n",
      " Processed 890/2904 files\n",
      " Processed 900/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 10/30\n",
      "   Files 901 to 1000 of 2904\n",
      " Written 900,000 records to CSV\n",
      " Processed 910/2904 files\n",
      " Processed 920/2904 files\n",
      " Written 920,000 records to CSV\n",
      " Processed 930/2904 files\n",
      " Processed 940/2904 files\n",
      " Written 940,000 records to CSV\n",
      " Processed 950/2904 files\n",
      " Processed 960/2904 files\n",
      " Written 960,000 records to CSV\n",
      " Processed 970/2904 files\n",
      " Processed 980/2904 files\n",
      " Written 980,000 records to CSV\n",
      " Processed 990/2904 files\n",
      " Processed 1000/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 11/30\n",
      "   Files 1001 to 1100 of 2904\n",
      " Written 1,000,000 records to CSV\n",
      " Processed 1010/2904 files\n",
      " Processed 1020/2904 files\n",
      " Written 1,020,000 records to CSV\n",
      " Processed 1030/2904 files\n",
      " Processed 1040/2904 files\n",
      " Written 1,040,000 records to CSV\n",
      " Processed 1050/2904 files\n",
      " Processed 1060/2904 files\n",
      " Written 1,060,000 records to CSV\n",
      " Processed 1070/2904 files\n",
      " Processed 1080/2904 files\n",
      " Written 1,080,000 records to CSV\n",
      " Processed 1090/2904 files\n",
      " Processed 1100/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 12/30\n",
      "   Files 1101 to 1200 of 2904\n",
      " Written 1,100,000 records to CSV\n",
      " Failed to process record 558: 'NoneType' object has no attribute 'get'\n",
      " Processed 1110/2904 files\n",
      " Processed 1120/2904 files\n",
      " Written 1,120,000 records to CSV\n",
      " Processed 1130/2904 files\n",
      " Processed 1140/2904 files\n",
      " Written 1,140,000 records to CSV\n",
      " Processed 1150/2904 files\n",
      " Processed 1160/2904 files\n",
      " Written 1,160,000 records to CSV\n",
      " Processed 1170/2904 files\n",
      " Processed 1180/2904 files\n",
      " Written 1,180,000 records to CSV\n",
      " Processed 1190/2904 files\n",
      " Failed to process record 342: 'NoneType' object has no attribute 'get'\n",
      " Processed 1200/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 13/30\n",
      "   Files 1201 to 1300 of 2904\n",
      " Written 1,200,000 records to CSV\n",
      " Processed 1210/2904 files\n",
      " Processed 1220/2904 files\n",
      " Written 1,220,000 records to CSV\n",
      " Processed 1230/2904 files\n",
      " Processed 1240/2904 files\n",
      " Written 1,240,000 records to CSV\n",
      " Processed 1250/2904 files\n",
      " Processed 1260/2904 files\n",
      " Written 1,260,000 records to CSV\n",
      " Processed 1270/2904 files\n",
      " Processed 1280/2904 files\n",
      " Written 1,280,000 records to CSV\n",
      " Processed 1290/2904 files\n",
      " Processed 1300/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 14/30\n",
      "   Files 1301 to 1400 of 2904\n",
      " Written 1,300,000 records to CSV\n",
      " Processed 1310/2904 files\n",
      " Processed 1320/2904 files\n",
      " Written 1,320,000 records to CSV\n",
      " Processed 1330/2904 files\n",
      " Processed 1340/2904 files\n",
      " Written 1,340,000 records to CSV\n",
      " Processed 1350/2904 files\n",
      " Processed 1360/2904 files\n",
      " Written 1,360,000 records to CSV\n",
      " Processed 1370/2904 files\n",
      " Processed 1380/2904 files\n",
      " Written 1,380,000 records to CSV\n",
      " Processed 1390/2904 files\n",
      " Processed 1400/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 15/30\n",
      "   Files 1401 to 1500 of 2904\n",
      " Written 1,400,000 records to CSV\n",
      " Processed 1410/2904 files\n",
      " Processed 1420/2904 files\n",
      " Written 1,420,000 records to CSV\n",
      " Processed 1430/2904 files\n",
      " Processed 1440/2904 files\n",
      " Written 1,440,000 records to CSV\n",
      " Processed 1450/2904 files\n",
      " Processed 1460/2904 files\n",
      " Written 1,460,000 records to CSV\n",
      " Processed 1470/2904 files\n",
      " Processed 1480/2904 files\n",
      " Written 1,480,000 records to CSV\n",
      " Processed 1490/2904 files\n",
      " Processed 1500/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 16/30\n",
      "   Files 1501 to 1600 of 2904\n",
      " Written 1,500,000 records to CSV\n",
      " Processed 1510/2904 files\n",
      " Processed 1520/2904 files\n",
      " Written 1,520,000 records to CSV\n",
      " Processed 1530/2904 files\n",
      " Processed 1540/2904 files\n",
      " Written 1,540,000 records to CSV\n",
      " Processed 1550/2904 files\n",
      " Processed 1560/2904 files\n",
      " Written 1,560,000 records to CSV\n",
      " Processed 1570/2904 files\n",
      " Processed 1580/2904 files\n",
      " Written 1,580,000 records to CSV\n",
      " Processed 1590/2904 files\n",
      " Processed 1600/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 17/30\n",
      "   Files 1601 to 1700 of 2904\n",
      " Written 1,600,000 records to CSV\n",
      " Processed 1610/2904 files\n",
      " Processed 1620/2904 files\n",
      " Written 1,620,000 records to CSV\n",
      " Processed 1630/2904 files\n",
      " Processed 1640/2904 files\n",
      " Written 1,640,000 records to CSV\n",
      " Processed 1650/2904 files\n",
      " Processed 1660/2904 files\n",
      " Written 1,660,000 records to CSV\n",
      " Processed 1670/2904 files\n",
      " Processed 1680/2904 files\n",
      " Written 1,680,000 records to CSV\n",
      " Processed 1690/2904 files\n",
      " Processed 1700/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 18/30\n",
      "   Files 1701 to 1800 of 2904\n",
      " Written 1,700,000 records to CSV\n",
      " Processed 1710/2904 files\n",
      " Processed 1720/2904 files\n",
      " Written 1,720,000 records to CSV\n",
      " Processed 1730/2904 files\n",
      " Processed 1740/2904 files\n",
      " Written 1,740,000 records to CSV\n",
      " Processed 1750/2904 files\n",
      " Processed 1760/2904 files\n",
      " Written 1,760,000 records to CSV\n",
      " Processed 1770/2904 files\n",
      " Processed 1780/2904 files\n",
      " Written 1,780,000 records to CSV\n",
      " Processed 1790/2904 files\n",
      " Processed 1800/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 19/30\n",
      "   Files 1801 to 1900 of 2904\n",
      " Written 1,800,000 records to CSV\n",
      " Processed 1810/2904 files\n",
      " Processed 1820/2904 files\n",
      " Written 1,820,000 records to CSV\n",
      " Failed to process record 344: 'NoneType' object has no attribute 'get'\n",
      " Processed 1830/2904 files\n",
      " Processed 1840/2904 files\n",
      " Written 1,840,000 records to CSV\n",
      " Processed 1850/2904 files\n",
      " Processed 1860/2904 files\n",
      " Written 1,860,000 records to CSV\n",
      " Processed 1870/2904 files\n",
      " Processed 1880/2904 files\n",
      " Written 1,880,000 records to CSV\n",
      " Processed 1890/2904 files\n",
      " Processed 1900/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 20/30\n",
      "   Files 1901 to 2000 of 2904\n",
      " Written 1,900,000 records to CSV\n",
      " Processed 1910/2904 files\n",
      " Processed 1920/2904 files\n",
      " Written 1,920,000 records to CSV\n",
      " Processed 1930/2904 files\n",
      " Processed 1940/2904 files\n",
      " Written 1,940,000 records to CSV\n",
      " Processed 1950/2904 files\n",
      " Processed 1960/2904 files\n",
      " Written 1,960,000 records to CSV\n",
      " Processed 1970/2904 files\n",
      " Processed 1980/2904 files\n",
      " Written 1,980,000 records to CSV\n",
      " Processed 1990/2904 files\n",
      " Processed 2000/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 21/30\n",
      "   Files 2001 to 2100 of 2904\n",
      " Written 2,000,000 records to CSV\n",
      " Processed 2010/2904 files\n",
      " Processed 2020/2904 files\n",
      " Written 2,020,000 records to CSV\n",
      " Processed 2030/2904 files\n",
      " Processed 2040/2904 files\n",
      " Written 2,040,000 records to CSV\n",
      " Processed 2050/2904 files\n",
      " Processed 2060/2904 files\n",
      " Written 2,060,000 records to CSV\n",
      " Processed 2070/2904 files\n",
      " Processed 2080/2904 files\n",
      " Written 2,080,000 records to CSV\n",
      " Processed 2090/2904 files\n",
      " Failed to process record 893: 'NoneType' object has no attribute 'get'\n",
      " Processed 2100/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 22/30\n",
      "   Files 2101 to 2200 of 2904\n",
      " Written 2,100,000 records to CSV\n",
      " Processed 2110/2904 files\n",
      " Processed 2120/2904 files\n",
      " Written 2,120,000 records to CSV\n",
      " Processed 2130/2904 files\n",
      " Processed 2140/2904 files\n",
      " Written 2,140,000 records to CSV\n",
      " Processed 2150/2904 files\n",
      " Processed 2160/2904 files\n",
      " Written 2,160,000 records to CSV\n",
      " Processed 2170/2904 files\n",
      " Processed 2180/2904 files\n",
      " Written 2,180,000 records to CSV\n",
      " Processed 2190/2904 files\n",
      " Processed 2200/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 23/30\n",
      "   Files 2201 to 2300 of 2904\n",
      " Written 2,200,000 records to CSV\n",
      " Processed 2210/2904 files\n",
      " Processed 2220/2904 files\n",
      " Written 2,220,000 records to CSV\n",
      " Processed 2230/2904 files\n",
      " Processed 2240/2904 files\n",
      " Written 2,240,000 records to CSV\n",
      " Processed 2250/2904 files\n",
      " Processed 2260/2904 files\n",
      " Written 2,260,000 records to CSV\n",
      " Processed 2270/2904 files\n",
      " Processed 2280/2904 files\n",
      " Written 2,280,000 records to CSV\n",
      " Processed 2290/2904 files\n",
      " Processed 2300/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 24/30\n",
      "   Files 2301 to 2400 of 2904\n",
      " Written 2,300,000 records to CSV\n",
      " Processed 2310/2904 files\n",
      " Processed 2320/2904 files\n",
      " Written 2,320,000 records to CSV\n",
      " Processed 2330/2904 files\n",
      " Processed 2340/2904 files\n",
      " Written 2,340,000 records to CSV\n",
      " Processed 2350/2904 files\n",
      " Processed 2360/2904 files\n",
      " Written 2,360,000 records to CSV\n",
      " Processed 2370/2904 files\n",
      " Processed 2380/2904 files\n",
      " Written 2,380,000 records to CSV\n",
      " Processed 2390/2904 files\n",
      " Processed 2400/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 25/30\n",
      "   Files 2401 to 2500 of 2904\n",
      " Written 2,400,000 records to CSV\n",
      " Processed 2410/2904 files\n",
      " Processed 2420/2904 files\n",
      " Written 2,420,000 records to CSV\n",
      " Processed 2430/2904 files\n",
      " Processed 2440/2904 files\n",
      " Written 2,440,000 records to CSV\n",
      " Processed 2450/2904 files\n",
      " Processed 2460/2904 files\n",
      " Written 2,460,000 records to CSV\n",
      " Processed 2470/2904 files\n",
      " Processed 2480/2904 files\n",
      " Written 2,480,000 records to CSV\n",
      " Processed 2490/2904 files\n",
      " Processed 2500/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 26/30\n",
      "   Files 2501 to 2600 of 2904\n",
      " Written 2,500,000 records to CSV\n",
      " Processed 2510/2904 files\n",
      " Processed 2520/2904 files\n",
      " Written 2,520,000 records to CSV\n",
      " Processed 2530/2904 files\n",
      " Processed 2540/2904 files\n",
      " Written 2,540,000 records to CSV\n",
      " Processed 2550/2904 files\n",
      " Processed 2560/2904 files\n",
      " Written 2,560,000 records to CSV\n",
      " Processed 2570/2904 files\n",
      " Processed 2580/2904 files\n",
      " Written 2,580,000 records to CSV\n",
      " Processed 2590/2904 files\n",
      " Processed 2600/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 27/30\n",
      "   Files 2601 to 2700 of 2904\n",
      " Written 2,600,000 records to CSV\n",
      " Processed 2610/2904 files\n",
      " Processed 2620/2904 files\n",
      " Written 2,620,000 records to CSV\n",
      " Processed 2630/2904 files\n",
      " Processed 2640/2904 files\n",
      " Written 2,640,000 records to CSV\n",
      " Processed 2650/2904 files\n",
      " Processed 2660/2904 files\n",
      " Written 2,660,000 records to CSV\n",
      " Processed 2670/2904 files\n",
      " Processed 2680/2904 files\n",
      " Written 2,680,000 records to CSV\n",
      " Processed 2690/2904 files\n",
      " Processed 2700/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 28/30\n",
      "   Files 2701 to 2800 of 2904\n",
      " Written 2,700,000 records to CSV\n",
      " Processed 2710/2904 files\n",
      " Processed 2720/2904 files\n",
      " Written 2,720,000 records to CSV\n",
      " Processed 2730/2904 files\n",
      " Processed 2740/2904 files\n",
      " Written 2,740,000 records to CSV\n",
      " Processed 2750/2904 files\n",
      " Processed 2760/2904 files\n",
      " Written 2,760,000 records to CSV\n",
      " Processed 2770/2904 files\n",
      " Processed 2780/2904 files\n",
      " Written 2,780,000 records to CSV\n",
      " Processed 2790/2904 files\n",
      " Processed 2800/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 29/30\n",
      "   Files 2801 to 2900 of 2904\n",
      " Written 2,800,000 records to CSV\n",
      " Processed 2810/2904 files\n",
      " Processed 2820/2904 files\n",
      " Written 2,820,000 records to CSV\n",
      " Processed 2830/2904 files\n",
      " Processed 2840/2904 files\n",
      " Written 2,840,000 records to CSV\n",
      " Processed 2850/2904 files\n",
      " Processed 2860/2904 files\n",
      " Written 2,860,000 records to CSV\n",
      " Processed 2870/2904 files\n",
      " Processed 2880/2904 files\n",
      " Written 2,880,000 records to CSV\n",
      " Processed 2890/2904 files\n",
      " Processed 2900/2904 files\n",
      "\n",
      "ðŸ“¦ Processing batch 30/30\n",
      "   Files 2901 to 2904 of 2904\n",
      "\n",
      "âœ“ Written final 19662 records\n",
      "\n",
      "============================================================\n",
      " PROCESSING COMPLETE\n",
      "Total records processed: 2,899,662\n",
      "Failed files: 0\n",
      "Failed records: 11\n",
      "Output saved to: //bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_chars/complete_charmetric_chars.csv\n",
      "============================================================\n",
      "Error log saved to: //bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_chars/complete_charmetric_chars_errors.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Cell 6: Run the processing\n",
    "total_processed, errors_files, errors_records = process_batch_files_incrementally()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Alternative - Parquet output (more efficient for large datasets)\n",
    "def process_to_parquet_chunks():\n",
    "    \"\"\"Process to Parquet format in chunks\"\"\"\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    \n",
    "    print(f\"Processing {len(all_batch_files)} files to Parquet format\")\n",
    "    \n",
    "    writer = None\n",
    "    chunk_data = []\n",
    "    total_processed = 0\n",
    "    chunk_size = 50000\n",
    "    \n",
    "    try:\n",
    "        for file_idx, file_path in enumerate(all_batch_files):\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    batch_data = json.load(f)\n",
    "                \n",
    "                for record in batch_data:\n",
    "                    try:\n",
    "                        extracted = extract_song_info_dict(record)\n",
    "                        chunk_data.append(extracted)\n",
    "                        \n",
    "                        if len(chunk_data) >= chunk_size:\n",
    "                            df_chunk = pd.DataFrame(chunk_data)\n",
    "                            \n",
    "                            if writer is None:\n",
    "                                table = pa.Table.from_pandas(df_chunk)\n",
    "                                writer = pq.ParquetWriter(OUTPUT_PARQUET_PATH, table.schema)\n",
    "                            \n",
    "                            table = pa.Table.from_pandas(df_chunk)\n",
    "                            writer.write_table(table)\n",
    "                            \n",
    "                            total_processed += len(chunk_data)\n",
    "                            print(f\"Processed {total_processed:,} records\")\n",
    "                            chunk_data = []\n",
    "                            \n",
    "                            del df_chunk, table\n",
    "                            gc.collect()\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Failed to process record: {e}\")\n",
    "                \n",
    "                del batch_data\n",
    "                \n",
    "                if (file_idx + 1) % 10 == 0:\n",
    "                    print(f\"Processed {file_idx + 1}/{len(all_batch_files)} files\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Write remaining data\n",
    "        if chunk_data:\n",
    "            df_chunk = pd.DataFrame(chunk_data)\n",
    "            if writer is None:\n",
    "                table = pa.Table.from_pandas(df_chunk)\n",
    "                writer = pq.ParquetWriter(OUTPUT_PARQUET_PATH, table.schema)\n",
    "            else:\n",
    "                table = pa.Table.from_pandas(df_chunk)\n",
    "                writer.write_table(table)\n",
    "            total_processed += len(chunk_data)\n",
    "        \n",
    "        print(f\"\\nTotal records processed: {total_processed:,}\")\n",
    "        \n",
    "    finally:\n",
    "        if writer:\n",
    "            writer.close()\n",
    "    \n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the musicbrainz data with the characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the chartmetric_ids_spotify data as a dataframe\n",
    "chartmetric_ids_spotify_mb = pd.read_csv(\n",
    "    \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/chartmetric_ids_mb_matched.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_extracted_df = pd.read_csv(OUTPUT_CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Join the Extracted Data to the Original DataFrame ---\n",
    "# It is assumed that the 'id' column in song_chars_extracted matches the 'id' column in spotify_sample.\n",
    "merged_song_chars = chartmetric_ids_spotify_mb.merge(final_extracted_df, on=\"chartmetric_ids\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- REWORKING THE DATAFRAME ---\n",
    "\n",
    "# dropping columns\n",
    "#- spotify url\n",
    "# - artist booking agent\n",
    "# - artist general manager\n",
    "\n",
    "merged_song_chars = merged_song_chars.drop(columns=[\n",
    "    'spotify_url',\n",
    "    'Artist_booking_agent',\n",
    "    'Artist_general_manager'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#rename columns\n",
    "# release_date -> spotify_release_date\n",
    "# name_recording -> mb_name_recording\n",
    "# name_artist_credit -> mb_name_artist_credit\n",
    "\n",
    "merged_song_chars = merged_song_chars.rename(columns={\n",
    "    'release_date': 'spotify_release_date',\n",
    "    'name_recording': 'mb_name_recording',\n",
    "    'name_artist_credit': 'mb_name_artist_credit'\n",
    "})\n",
    "\n",
    "\n",
    "# reorder columns\n",
    "# id_release, id_track, id_recording, id_artist_credit, mbid_track, mbid_recording, spotify_track_id, spotify_isrc, chartmetric_ids\n",
    "# mb_name_recording, cm_track_title, spotify_track_title, mb_name_artist_credit, spotify_artist_credit, cm_artist_credit, spotify_album_name, cm_name_release\n",
    "# spotify_release_date, cm_release_date, cm_artist_label, cm_album_label, name_medium_format\n",
    "# cm_genres, cm_moods, cm_activities, cm_songwriters, cm_songwriterIds, cm_tempo, cm_duration_ms\n",
    "# rest\n",
    "\n",
    "merged_song_chars = merged_song_chars[[\n",
    "    'id_release', 'id_track', 'id_recording', 'id_artist_credit',\n",
    "    'mbid_track', 'mbid_recording', 'spotify_track_id', 'spotify_isrc', 'chartmetric_ids',\n",
    "    'mb_name_recording', 'cm_track_title', 'spotify_track_title',\n",
    "    'mb_name_artist_credit', 'spotify_artist_name', 'cm_artist_credit',\n",
    "    'spotify_album_name', 'cm_name_release', 'spotify_release_date', 'cm_release_date',\n",
    "    'cm_artist_label', 'cm_albums_label', 'name_medium_format',\n",
    "    'cm_genres', 'cm_moods', 'cm_activities', 'cm_songwriters', 'cm_songwriterIds',\n",
    "    'cm_tempo', 'cm_duration_ms']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safe the final dataframe\n",
    "# Save as JSON (records-oriented with one JSON object per line)\n",
    "\n",
    "#sample dataset\n",
    "# merged_song_chars.to_json(\"Z:/Data_alexander/data/raw_data/chartmetric/chartmetric_chars/charmetric_chars_mb.json\", orient=\"records\", lines=True)\n",
    "\n",
    "#the songs + artist 1980-2000 dataset\n",
    "merged_song_chars.to_json(\"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_final_merged/mb_1980_2000_charmetric_chars.json\", orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge the song characteristics to the chart songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the chartmetric_ids_spotify data as a dataframe\n",
    "chartmetric_ids_spotify_charts = pd.read_csv(\n",
    "    \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/chartmetric_ids_chart_songs_matched.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a left join \n",
    "# --- Step 3: Join the Extracted Data to the Original DataFrame ---\n",
    "# It is assumed that the 'id' column in song_chars_extracted matches the 'id' column in spotify_sample.\n",
    "merged_song_chars_charts = chartmetric_ids_spotify_charts.merge(final_extracted_df, on=\"chartmetric_ids\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- REWORKING THE DATAFRAME ---\n",
    "\n",
    "# dropping columns\n",
    "#- spotify url\n",
    "# - artist booking agent\n",
    "# - artist general manager\n",
    "\n",
    "merged_song_chars_charts = merged_song_chars_charts.drop(columns=[\n",
    "    'spotify_url',\n",
    "    'Artist_booking_agent',\n",
    "    'Artist_general_manager'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safe the final dataframe\n",
    "# Save as JSON (records-oriented with one JSON object per line)\n",
    "\n",
    "#the songs + artist 1980-2000 dataset\n",
    "merged_song_chars_charts.to_json(\"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_final_merged/charts_1980_2000_charmetric_chars.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "name_recording             0.004160\n",
      "name_artist_credit         0.004160\n",
      "song_artist                2.113145\n",
      "label                     69.030782\n",
      "artist_song               97.886855\n",
      "country                   13.294509\n",
      "tf_name_recording          0.004160\n",
      "tf_name_artist_credit      0.004160\n",
      "cm_track_title             0.303661\n",
      "cm_composer_name          33.173877\n",
      "cm_artist_id               0.320300\n",
      "cm_artist_credit           0.320300\n",
      "cm_artist_label          100.000000\n",
      "cm_release_id              0.374376\n",
      "cm_name_release            0.374376\n",
      "cm_release_date            0.374376\n",
      "cm_albums_label            0.682196\n",
      "cm_genres                  0.303661\n",
      "cm_moods                  19.147255\n",
      "cm_activities             55.732113\n",
      "cm_songwriters            33.173877\n",
      "cm_songwriterIds         100.000000\n",
      "cm_tempo                  13.057404\n",
      "cm_duration_ms             0.611481\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_counts = merged_song_chars_charts.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_counts[missing_counts > 0]/len(merged_song_chars_charts) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random sample of 100 rows and save as random_sample_songs_mb \n",
    "random_sample_songs_mb = merged_song_chars.sample(n=100, random_state=42)\n",
    "\n",
    "# same for the charts dataset\n",
    "random_sample_songs_charts = merged_song_chars_charts.sample(n=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trial the function with the search output\n",
    "\n",
    "test_df = extract_song_info(search_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_data_chartmetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
