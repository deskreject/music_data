{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t# Author: Alexander Staub\n",
    "\t## Last changed: 2025.06.26\n",
    "\t## Purpose: the template for each worker to access the chartmetric characteristics endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing packages\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the logging of the errors\n",
    "logging.basicConfig(\n",
    "    filename='chartmetric_api_metadata.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s: %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define API host and your refresh token\n",
    "HOST = 'https://api.chartmetric.com'\n",
    "with open(\"chartmetric_refresh_token.txt\", \"r\") as f:\n",
    "    REFRESH_TOKEN = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve an access token using the refresh token\n",
    "token_response = requests.post(f'{HOST}/api/token', json={'refreshtoken': REFRESH_TOKEN})\n",
    "\n",
    "# Check if the token was retrieved successfully\n",
    "if token_response.status_code != 200:\n",
    "\n",
    "    # Log the error and raise an exception\n",
    "    logging.error(f\"Token retrieval error: {token_response.status_code}\")\n",
    "    raise Exception(f\"Error: received {token_response.status_code} from /api/token\")\n",
    "\n",
    "# Extract the access token from the response\n",
    "access_token = token_response.json()['token']\n",
    "\n",
    "# Define the headers for the API requests\n",
    "headers = {'Authorization': f'Bearer {access_token}'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the get_request\n",
    "\n",
    "Robust request logic that:\n",
    "- backs off for a max of 26 hours in retries\n",
    "- logs all erros it encounters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Robust get_request Function ---\n",
    "def get_request(endpoint, params=None, max_retries=5):\n",
    "    backoff = 1  # initial backoff in seconds (used if header data is missing)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(f\"{HOST}{endpoint}\", headers=headers, params=params)\n",
    "        except Exception as ex:\n",
    "            logging.error(f\"Network error on attempt {attempt+1} for {endpoint}: {ex}\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "            continue\n",
    "\n",
    "# Log the response status code and rate limit headers\n",
    "        logging.info(f\"Request to {endpoint} returned {response.status_code}. RateLimit headers: {response.headers}\")\n",
    "\n",
    "# Check if the response status code is 200\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "\n",
    "# Handle different types of errors\n",
    "# 401: Token may have expired; refresh it\n",
    "        elif response.status_code == 401:\n",
    "            # Token may have expired; refresh it\n",
    "            logging.warning(f\"401 error for {endpoint}. Refreshing token.\")\n",
    "            token_response = requests.post(f'{HOST}/api/token', json={'refreshtoken': REFRESH_TOKEN})\n",
    "            if token_response.status_code != 200:\n",
    "                logging.error(f\"Token refresh failed: {token_response.status_code}\")\n",
    "                raise Exception(f\"Token refresh failed with status {token_response.status_code}\")\n",
    "            new_token = token_response.json()['token']\n",
    "            headers['Authorization'] = f'Bearer {new_token}'\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "\n",
    "# 429: Rate limit exceeded; wait and retry\n",
    "        elif response.status_code == 429:\n",
    "            # Rate limit exceeded.\n",
    "            reset_timestamp = response.headers.get(\"X-RateLimit-Reset\")\n",
    "            if reset_timestamp:\n",
    "                # Wait until the time provided by the API\n",
    "                sleep_time = int(reset_timestamp) - int(time.time())\n",
    "                if sleep_time < 0:\n",
    "                    sleep_time = backoff\n",
    "            else:\n",
    "                # No wait time provided by the API; compute one that totals 26 hours over all retries.\n",
    "                total_wait_limit = 26 * 3600  # total wait time in seconds (26 hours)\n",
    "                # Sum exponential weights for remaining attempts: for i from current attempt to max_retries-1\n",
    "                remaining_weights = sum(2 ** i for i in range(attempt, max_retries))\n",
    "                # Use the weight for the current attempt to assign a fraction of the total wait.\n",
    "                sleep_time = total_wait_limit * (2 ** attempt / remaining_weights)\n",
    "            logging.warning(f\"429 error for {endpoint}. Sleeping for {sleep_time} seconds (attempt {attempt+1}/{max_retries}).\")\n",
    "            time.sleep(sleep_time)\n",
    "            backoff *= 2\n",
    "\n",
    "# 500: Server error; wait and retry\n",
    "        elif response.status_code >= 500:\n",
    "            logging.warning(f\"Server error {response.status_code} for {endpoint}. Retrying after {backoff} seconds.\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "\n",
    "        else:\n",
    "            logging.error(f\"Error {response.status_code} for {endpoint}: {response.text}\")\n",
    "            raise Exception(f\"Error: received {response.status_code} from {endpoint}\")\n",
    "\n",
    "# If the loop completes without returning, raise an exception\n",
    "    raise Exception(f\"Max retries exceeded for endpoint {endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to retreive the song metadata from chartmetrics\n",
    "\n",
    "- create the get request\n",
    "- run the loop over each chartmetric id\n",
    "- save the response for later parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to Retrieve song characteristics from Chartmetric ID ---\n",
    "def get_songchars_ids(chartmetric_id):\n",
    "    endpoint = f\"/api/track/{chartmetric_id}\"\n",
    "    try:\n",
    "        response = get_request(endpoint)\n",
    "        logging.info(f\"Successfully retrieved song chars for Chartmetric ID {chartmetric_id}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to get song chars for Chartmetric ID {chartmetric_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # The API response (a dictionary) is returned as is\n",
    "    song_chars = response\n",
    "    return song_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 7: WORKER CONFIGURATION AND EXECUTION ---\n",
    "# This cell replaces your original main processing loop.\n",
    "\n",
    "# ================== WORKER-SPECIFIC CONFIGURATION ==================\n",
    "# --- CHANGE: This is the ONLY line you will edit in each copied worker notebook. ---\n",
    "PART_NUMBER = 3  # For worker 1, set to 1. For worker 2, set to 2, etc.\n",
    "# ===================================================================\n",
    "\n",
    "\n",
    "# --- Main Processing with Checkpointing & DYNAMIC Throttling ---\n",
    "TOTAL_RATE_LIMIT = 3.5\n",
    "NUM_WORKERS = 3  # Must match the controller script\n",
    "# Each worker gets an equal share of the rate limit.\n",
    "TIME_PER_REQUEST = 1 / (TOTAL_RATE_LIMIT / NUM_WORKERS) \n",
    "\n",
    "### WHAT THIS CHANGE DOES:\n",
    "# This is the most important change for avoiding rate limits. It ensures that the sum of all\n",
    "# workers' requests does not exceed your total allowance.\n",
    "\n",
    "Of course. This is a perfect use case for parallel processing. Running multiple scripts is the correct way to utilize your approved rate limit when individual API calls are slow.\n",
    "\n",
    "Here is a complete guide to refactoring your Jupyter Notebook into a robust, 3-worker parallel processing system. I will first outline the potential issues as you requested, and then detail every necessary change to your notebook code.\n",
    "Thinking Through the Issues: A Pre-Analysis\n",
    "\n",
    "Before we change the code, let's address the challenges and design a system to handle them:\n",
    "\n",
    "    Rate Limiting (The Most Critical Issue): You are allowed 3.5 requests/second in total across all scripts. If you run 3 workers and each one tries to hit 3.5 req/s, your IP address will be sending over 10 req/s. You will be instantly rate-limited (429 errors), and your get_request function's backoff logic will kick in, grinding everything to a halt.\n",
    "        Solution: We must divide the rate limit. Each of the 3 workers will be configured to target only its share of the total rate (i.e., 3.5 / 3 â‰ˆ 1.17 req/s each). This ensures the combined rate stays below the limit.\n",
    "\n",
    "    Duplicate Queries & State Management: Your concern about duplicate queries is valid. If three scripts read the same master list, they will all request the same IDs. If they write to the same checkpoint file, they will corrupt it by overwriting each other's progress (a \"race condition\").\n",
    "        Solution: We will use a \"Controller/Worker\" pattern. A single \"Controller\" script will pre-process the master list of IDs, splitting it into 3 exclusive, non-overlapping files. Each of the 3 \"Worker\" scripts will then be assigned one file. This completely eliminates the possibility of duplicate queries or race conditions, as they operate in separate sandboxes.\n",
    "\n",
    "    Robust Checkpointing & Resuming: Your current checkpoint logic overwrites the file every 100 rows. However, it doesn't have logic to load from the checkpoint and resume if the script is stopped and restarted.\n",
    "        Solution: We will significantly improve this. The worker script will now check if a checkpoint file exists upon starting. If it does, it will load the previously saved responses and skip to the correct starting row in its input file. This makes the entire process resilient to crashes or interruptions.\n",
    "\n",
    "    Jupyter Notebook Environment: Running parallel tasks from a single notebook file is complex. The strategy of using separate files (worker_1.ipynb, worker_2.ipynb, etc.) is the most practical approach.\n",
    "        Solution: The guide will show you how to create one master template (worker_template.ipynb) and then how to run it as three separate processes from your command line.\n",
    "\n",
    "Here is the step-by-step refactoring of your notebook.\n",
    "Part 1: The Controller Notebook (00_controller.ipynb)\n",
    "\n",
    "Create a new, separate notebook with this code. Its only purpose is to prepare the data for your workers. You only need to run it once.\n",
    "Python\n",
    "\n",
    "# FILE: 00_controller.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "NUM_WORKERS = 3  # The number of parallel scripts you want to run\n",
    "# --- CHANGE: Define the path to your full, original dataset ---\n",
    "MASTER_INPUT_FILE = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/chartmetric_ids_sample.csv\"\n",
    "# --- CHANGE: Define a new directory where the split input files will be saved ---\n",
    "WORKER_INPUT_DIR = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/worker_inputs/\"\n",
    "\n",
    "### WHAT THIS CHANGE DOES:\n",
    "# This separates the logic. This notebook acts as the main setup script.\n",
    "# It defines a new, dedicated folder to hold the input files for each worker, keeping your project organized.\n",
    "\n",
    "print(f\"Creating worker input directory: {WORKER_INPUT_DIR}\")\n",
    "os.makedirs(WORKER_INPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Load and Prepare the Master Dataset ---\n",
    "print(f\"Loading master dataset from {MASTER_INPUT_FILE}...\")\n",
    "master_df = pd.read_csv(MASTER_INPUT_FILE)\n",
    "\n",
    "# Clean the data ONCE before splitting\n",
    "master_df = master_df.drop_duplicates(subset=\"chartmetric_ids\")\n",
    "master_df.dropna(subset=['chartmetric_ids'], inplace=True)\n",
    "master_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Loaded and cleaned {len(master_df)} unique IDs.\")\n",
    "\n",
    "# --- Split the DataFrame into Chunks for Each Worker ---\n",
    "id_chunks = np.array_split(master_df, NUM_WORKERS)\n",
    "\n",
    "# --- Save Each Chunk to its Own File ---\n",
    "for i, chunk in enumerate(id_chunks):\n",
    "    part_number = i + 1\n",
    "    # --- CHANGE: The output path is now dynamic for each worker part. ---\n",
    "    output_path = os.path.join(WORKER_INPUT_DIR, f\"ids_part_{part_number}.csv\")\n",
    "    \n",
    "    ### WHAT THIS CHANGE DOES:\n",
    "    # It creates separate, numbered input files (e.g., ids_part_1.csv, ids_part_2.csv).\n",
    "    # Each file contains a unique and non-overlapping subset of the original IDs.\n",
    "    \n",
    "    chunk.to_csv(output_path, index=False)\n",
    "    print(f\"Saved chunk {part_number} with {len(chunk)} IDs to {output_path}\")\n",
    "\n",
    "print(\"\\nController script finished. You can now run the worker notebooks.\")\n",
    "\n",
    "Part 2: The Worker Notebook Template (worker_template.ipynb)\n",
    "\n",
    "This is the refactored version of your main notebook. Save this as a template. You will make copies of it to run in parallel.\n",
    "Python\n",
    "\n",
    "# FILE: worker_template.ipynb\n",
    "\n",
    "# --- Cell 1: Imports and Logging ---\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os # <-- Make sure os is imported\n",
    "\n",
    "# --- Cell 2: Logging Configuration (UNMODIFIED) ---\n",
    "logging.basicConfig(\n",
    "    filename='chartmetric_api_metadata.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s: %(message)s'\n",
    ")\n",
    "\n",
    "# --- Cell 3: API Host and Token (UNMODIFIED) ---\n",
    "HOST = 'https://api.chartmetric.com'\n",
    "with open(\"chartmetric_refresh_token.txt\", \"r\") as f:\n",
    "    REFRESH_TOKEN = f.read().strip()\n",
    "\n",
    "# --- Cell 4: Retrieve Access Token (UNMODIFIED) ---\n",
    "token_response = requests.post(f'{HOST}/api/token', json={'refreshtoken': REFRESH_TOKEN})\n",
    "if token_response.status_code != 200:\n",
    "    logging.error(f\"Token retrieval error: {token_response.status_code}\")\n",
    "    raise Exception(f\"Error: received {token_response.status_code} from /api/token\")\n",
    "access_token = token_response.json()['token']\n",
    "headers = {'Authorization': f'Bearer {access_token}'}\n",
    "\n",
    "# --- Cell 5: get_request Function (UNMODIFIED) ---\n",
    "# This robust function is perfect as-is.\n",
    "def get_request(endpoint, params=None, max_retries=5):\n",
    "    # ... (your existing get_request function code goes here, no changes needed) ...\n",
    "    backoff = 1\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(f\"{HOST}{endpoint}\", headers=headers, params=params)\n",
    "        except Exception as ex:\n",
    "            logging.error(f\"Network error on attempt {attempt+1} for {endpoint}: {ex}\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "            continue\n",
    "        logging.info(f\"Request to {endpoint} returned {response.status_code}. RateLimit headers: {response.headers}\")\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        elif response.status_code == 401:\n",
    "            logging.warning(f\"401 error for {endpoint}. Refreshing token.\")\n",
    "            token_response = requests.post(f'{HOST}/api/token', json={'refreshtoken': REFRESH_TOKEN})\n",
    "            if token_response.status_code != 200:\n",
    "                logging.error(f\"Token refresh failed: {token_response.status_code}\")\n",
    "                raise Exception(f\"Token refresh failed with status {token_response.status_code}\")\n",
    "            new_token = token_response.json()['token']\n",
    "            headers['Authorization'] = f'Bearer {new_token}'\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "        elif response.status_code == 429:\n",
    "            reset_timestamp = response.headers.get(\"X-RateLimit-Reset\")\n",
    "            if reset_timestamp:\n",
    "                sleep_time = int(reset_timestamp) - int(time.time())\n",
    "                if sleep_time < 0:\n",
    "                    sleep_time = backoff\n",
    "            else:\n",
    "                total_wait_limit = 26 * 3600\n",
    "                remaining_weights = sum(2 ** i for i in range(attempt, max_retries))\n",
    "                sleep_time = total_wait_limit * (2 ** attempt / remaining_weights)\n",
    "            logging.warning(f\"429 error for {endpoint}. Sleeping for {sleep_time} seconds (attempt {attempt+1}/{max_retries}).\")\n",
    "            time.sleep(sleep_time)\n",
    "            backoff *= 2\n",
    "        elif response.status_code >= 500:\n",
    "            logging.warning(f\"Server error {response.status_code} for {endpoint}. Retrying after {backoff} seconds.\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "        else:\n",
    "            logging.error(f\"Error {response.status_code} for {endpoint}: {response.text}\")\n",
    "            raise Exception(f\"Error: received {response.status_code} from {endpoint}\")\n",
    "    raise Exception(f\"Max retries exceeded for endpoint {endpoint}\")\n",
    "\n",
    "\n",
    "# --- Cell 6: get_songchars_ids Function (UNMODIFIED) ---\n",
    "# This function is also perfect as-is.\n",
    "def get_songchars_ids(chartmetric_id):\n",
    "    # ... (your existing get_songchars_ids function code goes here, no changes needed) ...\n",
    "    endpoint = f\"/api/track/{chartmetric_id}\"\n",
    "    try:\n",
    "        response = get_request(endpoint)\n",
    "        logging.info(f\"Successfully retrieved song chars for Chartmetric ID {chartmetric_id}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to get song chars for Chartmetric ID {chartmetric_id}: {e}\")\n",
    "        return None\n",
    "    return response\n",
    "\n",
    "\n",
    "# --- Cell 7: WORKER CONFIGURATION AND EXECUTION ---\n",
    "# This cell replaces your original main processing loop.\n",
    "\n",
    "# ================== WORKER-SPECIFIC CONFIGURATION ==================\n",
    "# --- CHANGE: This is the ONLY line you will edit in each copied worker notebook. ---\n",
    "PART_NUMBER = 1  # For worker 1, set to 1. For worker 2, set to 2, etc.\n",
    "# ===================================================================\n",
    "\n",
    "# --- CHANGE: Dynamic rate limit calculation ---\n",
    "TOTAL_RATE_LIMIT = 3.5\n",
    "NUM_WORKERS = 3  # Must match the controller script\n",
    "# Each worker gets an equal share of the rate limit.\n",
    "TIME_PER_REQUEST = 1 / (TOTAL_RATE_LIMIT / NUM_WORKERS) \n",
    "### WHAT THIS CHANGE DOES:\n",
    "# This is the most important change for avoiding rate limits. It ensures that the sum of all\n",
    "# workers' requests does not exceed your total allowance.\n",
    "\n",
    "# Dynamic file path generation ---\n",
    "WORKER_INPUT_DIR = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/worker_inputs/\"\n",
    "METADATA_OUTPUT_DIR = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_chars/\"\n",
    "\n",
    "INPUT_FILE = os.path.join(WORKER_INPUT_DIR, f\"ids_part_{PART_NUMBER}.csv\")\n",
    "# Each worker gets its own output directory to prevent file conflicts\n",
    "CHECKPOINT_DIR = os.path.join(METADATA_OUTPUT_DIR, f\"part_{PART_NUMBER}\") \n",
    "CHECKPOINT_FILE = os.path.join(CHECKPOINT_DIR, \"song_chars_checkpoint.json\")\n",
    "\n",
    "### WHAT THIS CHANGE DOES:\n",
    "# This makes the script a reusable template. By changing only PART_NUMBER at the top,\n",
    "# the script automatically targets the correct input file (e.g., `ids_part_1.csv`)\n",
    "# and creates a unique, safe output directory (e.g., `.../part_1/`) for its checkpoints.\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- CHANGE: Load worker-specific data ---\n",
    "print(f\"WORKER {PART_NUMBER}: Loading data from {INPUT_FILE}\")\n",
    "worker_df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "\n",
    "# --- CHANGE: Robust Checkpoint Loading and Resuming ---\n",
    "song_chars_responses = []\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    print(f\"WORKER {PART_NUMBER}: Found existing checkpoint. Loading previous responses...\")\n",
    "    with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "        song_chars_responses = json.load(f)\n",
    "\n",
    "# Determine the starting row by the number of responses already saved\n",
    "start_row = len(song_chars_responses)\n",
    "print(f\"WORKER {PART_NUMBER}: Resuming from row {start_row} of {len(worker_df)}.\")\n",
    "### WHAT THIS CHANGE DOES:\n",
    "# This makes your script truly robust. If a worker stops for any reason, you can just\n",
    "# restart it, and it will load its progress and continue where it left off,\n",
    "# saving you from re-requesting thousands of IDs.\n",
    "\n",
    "# --- Main Processing Loop (with .iloc for resuming) ---\n",
    "checkpoint_interval = 100\n",
    "# Use .iloc[start_row:] to slice the dataframe and start from the correct place\n",
    "for idx, row in worker_df.iloc[start_row:].iterrows():\n",
    "    loop_start_time = time.time()\n",
    "\n",
    "    chartmetric_id = row.get(\"chartmetric_ids\")\n",
    "    # --- Add worker ID to logging for clarity ---\n",
    "    print(f\"WORKER {PART_NUMBER} | Processing row {idx}: Chartmetric ID = {chartmetric_id}\")\n",
    "    logging.info(f\"WORKER {PART_NUMBER} | Processing row {idx}: Chartmetric ID = {chartmetric_id}\")\n",
    "\n",
    "    # No need to check for pd.isnull, the controller script already dropped them.\n",
    "    \n",
    "    if pd.isnull(chartmetric_id):\n",
    "        print(f\"Row {idx} has no Chartmetric ID. Skipping.\")\n",
    "        logging.info(f\"Row {idx} has no Chartmetric ID. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        song_chars = get_songchars_ids(chartmetric_id)\n",
    "    except Exception as e:\n",
    "        # --- CHANGE: Add worker ID to logging for clarity ---\n",
    "        print(f\"WORKER {PART_NUMBER} | Error processing Chartmetric ID {chartmetric_id} at row {idx}: {e}\")\n",
    "        logging.error(f\"WORKER {PART_NUMBER} | Error processing Chartmetric ID {chartmetric_id} at row {idx}: {e}\")\n",
    "        song_chars = None\n",
    "    \n",
    "    # Append the response (or None) to our list.\n",
    "    song_chars_responses.append(song_chars)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # --- DYNAMIC SLEEP LOGIC ---\n",
    "    # REMOVED: time.sleep(0.3)\n",
    "\n",
    "    # Calculate how long the API call and processing took\n",
    "    loop_end_time = time.time()\n",
    "    elapsed_time = loop_end_time - loop_start_time\n",
    "\n",
    "    # Calculate the remaining time to sleep to hit the target rate\n",
    "    sleep_duration = TIME_PER_REQUEST - elapsed_time\n",
    "\n",
    "    # If the request was fast, sleep for the remaining time.\n",
    "    # If the request was slow (elapsed_time > TIME_PER_REQUEST), don't sleep at all.\n",
    "    if sleep_duration > 0:\n",
    "        time.sleep(sleep_duration)\n",
    "    \n",
    "    # --- Checkpointing Logic ---\n",
    "    # We now check based on the length of the response list\n",
    "    if len(song_chars_responses) % checkpoint_interval == 0:\n",
    "        with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "            json.dump(song_chars_responses, f, indent=2)\n",
    "        print(f\"WORKER {PART_NUMBER} | Checkpoint saved at row {idx}\")\n",
    "        logging.info(f\"WORKER {PART_NUMBER} | Checkpoint saved at row {idx}\")\n",
    "\n",
    "# --- Final Save ---\n",
    "print(f\"WORKER {PART_NUMBER}: Loop finished. Saving final data...\")\n",
    "with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "    json.dump(song_chars_responses, f, indent=2)\n",
    "print(f\"WORKER {PART_NUMBER}: All tasks complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_music_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
