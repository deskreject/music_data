{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t# Author: Alexander Staub\n",
    "\t## Last changed: 2025.06.26\n",
    "\t## Purpose: the template for each worker to access the chartmetric characteristics endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing packages\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the logging of the errors\n",
    "logging.basicConfig(\n",
    "    filename='chartmetric_api_metadata_worker_2.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s: %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define API host and your refresh token\n",
    "HOST = 'https://api.chartmetric.com'\n",
    "with open(\"chartmetric_refresh_token.txt\", \"r\") as f:\n",
    "    REFRESH_TOKEN = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve an access token using the refresh token\n",
    "token_response = requests.post(f'{HOST}/api/token', json={'refreshtoken': REFRESH_TOKEN})\n",
    "\n",
    "# Check if the token was retrieved successfully\n",
    "if token_response.status_code != 200:\n",
    "\n",
    "    # Log the error and raise an exception\n",
    "    logging.error(f\"Token retrieval error: {token_response.status_code}\")\n",
    "    raise Exception(f\"Error: received {token_response.status_code} from /api/token\")\n",
    "\n",
    "# Extract the access token from the response\n",
    "access_token = token_response.json()['token']\n",
    "\n",
    "# Define the headers for the API requests\n",
    "headers = {'Authorization': f'Bearer {access_token}'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the get_request\n",
    "\n",
    "Robust request logic that:\n",
    "- backs off for a max of 26 hours in retries\n",
    "- logs all erros it encounters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Robust get_request Function ---\n",
    "def get_request(endpoint, params=None, max_retries=5):\n",
    "    backoff = 1  # initial backoff in seconds (used if header data is missing)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(f\"{HOST}{endpoint}\", headers=headers, params=params)\n",
    "        except Exception as ex:\n",
    "            logging.error(f\"Network error on attempt {attempt+1} for {endpoint}: {ex}\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "            continue\n",
    "\n",
    "# Log the response status code and rate limit headers\n",
    "        logging.info(f\"Request to {endpoint} returned {response.status_code}. RateLimit headers: {response.headers}\")\n",
    "\n",
    "# Check if the response status code is 200\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "\n",
    "# Handle different types of errors\n",
    "# 401: Token may have expired; refresh it\n",
    "        elif response.status_code == 401:\n",
    "            # Token may have expired; refresh it\n",
    "            logging.warning(f\"401 error for {endpoint}. Refreshing token.\")\n",
    "            token_response = requests.post(f'{HOST}/api/token', json={'refreshtoken': REFRESH_TOKEN})\n",
    "            if token_response.status_code != 200:\n",
    "                logging.error(f\"Token refresh failed: {token_response.status_code}\")\n",
    "                raise Exception(f\"Token refresh failed with status {token_response.status_code}\")\n",
    "            new_token = token_response.json()['token']\n",
    "            headers['Authorization'] = f'Bearer {new_token}'\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "\n",
    "# 429: Rate limit exceeded; wait and retry\n",
    "        elif response.status_code == 429:\n",
    "            # Rate limit exceeded.\n",
    "            reset_timestamp = response.headers.get(\"X-RateLimit-Reset\")\n",
    "            if reset_timestamp:\n",
    "                # Wait until the time provided by the API\n",
    "                sleep_time = int(reset_timestamp) - int(time.time())\n",
    "                if sleep_time < 0:\n",
    "                    sleep_time = backoff\n",
    "            else:\n",
    "                # No wait time provided by the API; compute one that totals 26 hours over all retries.\n",
    "                total_wait_limit = 26 * 3600  # total wait time in seconds (26 hours)\n",
    "                # Sum exponential weights for remaining attempts: for i from current attempt to max_retries-1\n",
    "                remaining_weights = sum(2 ** i for i in range(attempt, max_retries))\n",
    "                # Use the weight for the current attempt to assign a fraction of the total wait.\n",
    "                sleep_time = total_wait_limit * (2 ** attempt / remaining_weights)\n",
    "            logging.warning(f\"429 error for {endpoint}. Sleeping for {sleep_time} seconds (attempt {attempt+1}/{max_retries}).\")\n",
    "            time.sleep(sleep_time)\n",
    "            backoff *= 2\n",
    "\n",
    "# 500: Server error; wait and retry\n",
    "        elif response.status_code >= 500:\n",
    "            logging.warning(f\"Server error {response.status_code} for {endpoint}. Retrying after {backoff} seconds.\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "\n",
    "        else:\n",
    "            logging.error(f\"Error {response.status_code} for {endpoint}: {response.text}\")\n",
    "            raise Exception(f\"Error: received {response.status_code} from {endpoint}\")\n",
    "\n",
    "# If the loop completes without returning, raise an exception\n",
    "    raise Exception(f\"Max retries exceeded for endpoint {endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to retreive the song metadata from chartmetrics\n",
    "\n",
    "- create the get request\n",
    "- run the loop over each chartmetric id\n",
    "- save the response for later parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to Retrieve song characteristics from Chartmetric ID ---\n",
    "def get_songchars_ids(chartmetric_id):\n",
    "    endpoint = f\"/api/track/{chartmetric_id}\"\n",
    "    try:\n",
    "        response = get_request(endpoint)\n",
    "        logging.info(f\"Successfully retrieved song chars for Chartmetric ID {chartmetric_id}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to get song chars for Chartmetric ID {chartmetric_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # The API response (a dictionary) is returned as is\n",
    "    song_chars = response\n",
    "    return song_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 7: WORKER CONFIGURATION AND EXECUTION ---\n",
    "# This cell replaces your original main processing loop.\n",
    "\n",
    "# ================== WORKER-SPECIFIC CONFIGURATION ==================\n",
    "# --- CHANGE: This is the ONLY line you will edit in each copied worker notebook. ---\n",
    "PART_NUMBER = 2  # For worker 1, set to 1. For worker 2, set to 2, etc.\n",
    "# ===================================================================\n",
    "\n",
    "# --- CHANGE: Dynamic rate limit calculation ---\n",
    "TOTAL_RATE_LIMIT = 3.5\n",
    "NUM_WORKERS = 3  # Must match the controller script\n",
    "# Each worker gets an equal share of the rate limit.\n",
    "TIME_PER_REQUEST = 1 / (TOTAL_RATE_LIMIT / NUM_WORKERS) \n",
    "### WHAT THIS CHANGE DOES:\n",
    "# This is the most important change for avoiding rate limits. It ensures that the sum of all\n",
    "# workers' requests does not exceed your total allowance.\n",
    "\n",
    "# Dynamic file path generation ---\n",
    "WORKER_INPUT_DIR = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/worker_inputs/\"\n",
    "METADATA_OUTPUT_DIR = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_chars/\"\n",
    "\n",
    "INPUT_FILE = os.path.join(WORKER_INPUT_DIR, f\"ids_part_{PART_NUMBER}.csv\")\n",
    "# Each worker gets its own output directory to prevent file conflicts\n",
    "CHECKPOINT_DIR = os.path.join(METADATA_OUTPUT_DIR, f\"part_{PART_NUMBER}\") \n",
    "CHECKPOINT_FILE = os.path.join(CHECKPOINT_DIR, \"song_chars_checkpoint.json\")\n",
    "\n",
    "### WHAT THIS CHANGE DOES:\n",
    "# This makes the script a reusable template. By changing only PART_NUMBER at the top,\n",
    "# the script automatically targets the correct input file (e.g., `ids_part_1.csv`)\n",
    "# and creates a unique, safe output directory (e.g., `.../part_1/`) for its checkpoints.\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- CHANGE: Load worker-specific data ---\n",
    "print(f\"WORKER {PART_NUMBER}: Loading data from {INPUT_FILE}\")\n",
    "worker_df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "\n",
    "# --- CHANGE: Robust Checkpoint Loading and Resuming ---\n",
    "song_chars_responses = []\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    print(f\"WORKER {PART_NUMBER}: Found existing checkpoint. Loading previous responses...\")\n",
    "    with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "        song_chars_responses = json.load(f)\n",
    "\n",
    "# Determine the starting row by the number of responses already saved\n",
    "start_row = len(song_chars_responses)\n",
    "print(f\"WORKER {PART_NUMBER}: Resuming from row {start_row} of {len(worker_df)}.\")\n",
    "### WHAT THIS CHANGE DOES:\n",
    "# This makes your script truly robust. If a worker stops for any reason, you can just\n",
    "# restart it, and it will load its progress and continue where it left off,\n",
    "# saving you from re-requesting thousands of IDs.\n",
    "\n",
    "# --- Main Processing Loop (with .iloc for resuming) ---\n",
    "checkpoint_interval = 100\n",
    "# Use .iloc[start_row:] to slice the dataframe and start from the correct place\n",
    "for idx, row in worker_df.iloc[start_row:].iterrows():\n",
    "    loop_start_time = time.time()\n",
    "\n",
    "    chartmetric_id = row.get(\"chartmetric_ids\")\n",
    "    # --- Add worker ID to logging for clarity ---\n",
    "    print(f\"WORKER {PART_NUMBER} | Processing row {idx}: Chartmetric ID = {chartmetric_id}\")\n",
    "    logging.info(f\"WORKER {PART_NUMBER} | Processing row {idx}: Chartmetric ID = {chartmetric_id}\")\n",
    "\n",
    "    # No need to check for pd.isnull, the controller script already dropped them.\n",
    "    \n",
    "    if pd.isnull(chartmetric_id):\n",
    "        print(f\"Row {idx} has no Chartmetric ID. Skipping.\")\n",
    "        logging.info(f\"Row {idx} has no Chartmetric ID. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        song_chars = get_songchars_ids(chartmetric_id)\n",
    "    except Exception as e:\n",
    "        # --- CHANGE: Add worker ID to logging for clarity ---\n",
    "        print(f\"WORKER {PART_NUMBER} | Error processing Chartmetric ID {chartmetric_id} at row {idx}: {e}\")\n",
    "        logging.error(f\"WORKER {PART_NUMBER} | Error processing Chartmetric ID {chartmetric_id} at row {idx}: {e}\")\n",
    "        song_chars = None\n",
    "    \n",
    "    # Append the response (or None) to our list.\n",
    "    song_chars_responses.append(song_chars)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # --- DYNAMIC SLEEP LOGIC ---\n",
    "    # REMOVED: time.sleep(0.3)\n",
    "\n",
    "    # Calculate how long the API call and processing took\n",
    "    loop_end_time = time.time()\n",
    "    elapsed_time = loop_end_time - loop_start_time\n",
    "\n",
    "    # Calculate the remaining time to sleep to hit the target rate\n",
    "    sleep_duration = TIME_PER_REQUEST - elapsed_time\n",
    "\n",
    "    # If the request was fast, sleep for the remaining time.\n",
    "    # If the request was slow (elapsed_time > TIME_PER_REQUEST), don't sleep at all.\n",
    "    if sleep_duration > 0:\n",
    "        time.sleep(sleep_duration)\n",
    "    \n",
    "    # --- Checkpointing Logic ---\n",
    "    # We now check based on the length of the response list\n",
    "    if len(song_chars_responses) % checkpoint_interval == 0:\n",
    "        with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "            json.dump(song_chars_responses, f, indent=2)\n",
    "        print(f\"WORKER {PART_NUMBER} | Checkpoint saved at row {idx}\")\n",
    "        logging.info(f\"WORKER {PART_NUMBER} | Checkpoint saved at row {idx}\")\n",
    "\n",
    "# --- Final Save ---\n",
    "print(f\"WORKER {PART_NUMBER}: Loop finished. Saving final data...\")\n",
    "with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "    json.dump(song_chars_responses, f, indent=2)\n",
    "print(f\"WORKER {PART_NUMBER}: All tasks complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_music_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
