{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t# Author: Alexander Staub\n",
    "\t## Last changed: 2025.06.26\n",
    "\t## Purpose: RUN FIRST - devide the relevant dataset into N parts (3 at the moment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing packages\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating worker input directory: //bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/worker_inputs/\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "\n",
    "NUM_WORKERS = 3  # The number of parallel scripts you want to run\n",
    "\n",
    "# ---  Define the path to your sample dataset ---\n",
    "#MASTER_INPUT_FILE = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/chartmetric_ids_sample.csv\"\n",
    "\n",
    "# --- CHANGE: Define the path to your full datasets of musicbrainz and charts ---\n",
    "MASTER_INPUT_FILE_MB = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/chartmetric_ids_mb_matched.csv\"\n",
    "MASTER_INPUT_FILE_CHARTS = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/chartmetric_ids_chart_songs_matched.csv\"\n",
    "\n",
    "# --- Define a new directory where the split input files will be saved ---\n",
    "WORKER_INPUT_DIR = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/worker_inputs/\"\n",
    "\n",
    "### WHAT THIS CHANGE DOES:\n",
    "# This separates the logic. This notebook acts as the main setup script.\n",
    "# It defines a new, dedicated folder to hold the input files for each worker, keeping your project organized.\n",
    "\n",
    "print(f\"Creating worker input directory: {WORKER_INPUT_DIR}\")\n",
    "os.makedirs(WORKER_INPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file called MASTER_INPUT_FILE which row binds the two input files\n",
    "\n",
    "# load the two input files\n",
    "df_mb = pd.read_csv(MASTER_INPUT_FILE_MB)\n",
    "df_charts = pd.read_csv(MASTER_INPUT_FILE_CHARTS)\n",
    "\n",
    "# remove the columns in df_charts that are not in df_mb\n",
    "df_charts = df_charts[df_mb.columns]\n",
    "\n",
    "#remove the columns in df_mb that are not in df_charts\n",
    "df_mb = df_mb[df_charts.columns]\n",
    "\n",
    "#row bind the two dataframes\n",
    "df_combined = pd.concat([df_mb, df_charts], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading master dataset from //bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids/chartmetric_ids_sample.csv...\n"
     ]
    }
   ],
   "source": [
    "# --- Load and Prepare the Master Dataset ---\n",
    "print(f\"Loading master dataset from {MASTER_INPUT_FILE}...\")\n",
    "master_df = df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and cleaned 4573 unique IDs.\n"
     ]
    }
   ],
   "source": [
    "# Clean the data ONCE before splitting\n",
    "master_df = master_df.drop_duplicates(subset=\"chartmetric_ids\")\n",
    "master_df.dropna(subset=['chartmetric_ids'], inplace=True)\n",
    "master_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Loaded and cleaned {len(master_df)} unique IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\music_data_chartmetric\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# --- Split the DataFrame into Chunks for Each Worker ---\n",
    "id_chunks = np.array_split(master_df, NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 1 with 1525 IDs to //bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/worker_inputs/ids_part_1.csv\n",
      "Saved chunk 2 with 1524 IDs to //bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/worker_inputs/ids_part_2.csv\n",
      "Saved chunk 3 with 1524 IDs to //bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/worker_inputs/ids_part_3.csv\n",
      "\n",
      "Controller script finished. You can now run the worker notebooks.\n"
     ]
    }
   ],
   "source": [
    "# --- Save Each Chunk to its Own File ---\n",
    "for i, chunk in enumerate(id_chunks):\n",
    "    part_number = i + 1\n",
    "    # --- The output path is now dynamic for each worker part. ---\n",
    "    output_path = os.path.join(WORKER_INPUT_DIR, f\"ids_part_{part_number}.csv\")\n",
    "    \n",
    "    ### WHAT THIS CHANGE DOES:\n",
    "    # It creates separate, numbered input files (e.g., ids_part_1.csv, ids_part_2.csv).\n",
    "    # Each file contains a unique and non-overlapping subset of the original IDs.\n",
    "    \n",
    "    chunk.to_csv(output_path, index=False)\n",
    "    print(f\"Saved chunk {part_number} with {len(chunk)} IDs to {output_path}\")\n",
    "\n",
    "print(\"\\nController script finished. You can now run the worker notebooks.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_data_chartmetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
