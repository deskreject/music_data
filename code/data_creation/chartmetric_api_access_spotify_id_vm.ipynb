{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t# Author: Alexander Staub\n",
    "\t## Last changed: 2025.02.13\n",
    "\t## Purpose: Getting the chartmetrics IDs for a list of songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing packages\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the logging of the errors\n",
    "logging.basicConfig(\n",
    "    filename='chartmetric_api.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s: %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define API host and your refresh token\n",
    "HOST = 'https://api.chartmetric.com'\n",
    "with open(\"chartmetric_refresh_token.txt\", \"r\") as f:\n",
    "    REFRESH_TOKEN = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve an access token using the refresh token\n",
    "token_response = requests.post(f'{HOST}/api/token', json={'refreshtoken': REFRESH_TOKEN})\n",
    "\n",
    "# Check if the token was retrieved successfully\n",
    "if token_response.status_code != 200:\n",
    "\n",
    "    # Log the error and raise an exception\n",
    "    logging.error(f\"Token retrieval error: {token_response.status_code}\")\n",
    "    raise Exception(f\"Error: received {token_response.status_code} from /api/token\")\n",
    "\n",
    "# Extract the access token from the response\n",
    "access_token = token_response.json()['token']\n",
    "\n",
    "# Define the headers for the API requests\n",
    "headers = {'Authorization': f'Bearer {access_token}'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the get_request\n",
    "\n",
    "Robust request logic that:\n",
    "- backs off for a max of 26 hours in retries\n",
    "- logs all erros it encounters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Robust get_request Function ---\n",
    "def get_request(endpoint, params=None, max_retries=5):\n",
    "    backoff = 1  # initial backoff in seconds (used if header data is missing)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(f\"{HOST}{endpoint}\", headers=headers, params=params)\n",
    "        except Exception as ex:\n",
    "            logging.error(f\"Network error on attempt {attempt+1} for {endpoint}: {ex}\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "            continue\n",
    "\n",
    "# Log the response status code and rate limit headers\n",
    "        logging.info(f\"Request to {endpoint} returned {response.status_code}. RateLimit headers: {response.headers}\")\n",
    "\n",
    "# Check if the response status code is 200\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "\n",
    "# Handle different types of errors\n",
    "# 401: Token may have expired; refresh it\n",
    "        elif response.status_code == 401:\n",
    "            # Token may have expired; refresh it\n",
    "            logging.warning(f\"401 error for {endpoint}. Refreshing token.\")\n",
    "            token_response = requests.post(f'{HOST}/api/token', json={'refreshtoken': REFRESH_TOKEN})\n",
    "            if token_response.status_code != 200:\n",
    "                logging.error(f\"Token refresh failed: {token_response.status_code}\")\n",
    "                raise Exception(f\"Token refresh failed with status {token_response.status_code}\")\n",
    "            new_token = token_response.json()['token']\n",
    "            headers['Authorization'] = f'Bearer {new_token}'\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "\n",
    "# 429: Rate limit exceeded; wait and retry\n",
    "        elif response.status_code == 429:\n",
    "            # Rate limit exceeded.\n",
    "            reset_timestamp = response.headers.get(\"X-RateLimit-Reset\")\n",
    "            if reset_timestamp:\n",
    "                # Wait until the time provided by the API\n",
    "                sleep_time = int(reset_timestamp) - int(time.time())\n",
    "                if sleep_time < 0:\n",
    "                    sleep_time = backoff\n",
    "            else:\n",
    "                # No wait time provided by the API; compute one that totals 26 hours over all retries.\n",
    "                total_wait_limit = 26 * 3600  # total wait time in seconds (26 hours)\n",
    "                # Sum exponential weights for remaining attempts: for i from current attempt to max_retries-1\n",
    "                remaining_weights = sum(2 ** i for i in range(attempt, max_retries))\n",
    "                # Use the weight for the current attempt to assign a fraction of the total wait.\n",
    "                sleep_time = total_wait_limit * (2 ** attempt / remaining_weights)\n",
    "            logging.warning(f\"429 error for {endpoint}. Sleeping for {sleep_time} seconds (attempt {attempt+1}/{max_retries}).\")\n",
    "            time.sleep(sleep_time)\n",
    "            backoff *= 2\n",
    "\n",
    "# 500: Server error; wait and retry\n",
    "        elif response.status_code >= 500:\n",
    "            logging.warning(f\"Server error {response.status_code} for {endpoint}. Retrying after {backoff} seconds.\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "\n",
    "        else:\n",
    "            logging.error(f\"Error {response.status_code} for {endpoint}: {response.text}\")\n",
    "            raise Exception(f\"Error: received {response.status_code} from {endpoint}\")\n",
    "\n",
    "# If the loop completes without returning, raise an exception\n",
    "    raise Exception(f\"Max retries exceeded for endpoint {endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use isrc to access song characteristics:\n",
    "- load in a subset of around 5000 of the spotify songs with spotify IDs\n",
    "- Loop over them to get the chartmetric IDs and any other further information accessible\n",
    "- use chartmetric IDs to get the information of relevance\n",
    "- setup code in a way that allows us to get information for our relevant songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the first 8000 lines from the file \"AD_spotify_accoustic_char_250k.csv\" in the data/raw_data/Spotify folder and call it spotify_fetch\n",
    "# the data directory is 2 directories down from the current directory\n",
    "# it should be a pandas dataframe and the csv has headers\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#after each time a set is loaded, offset the next set by the previous set's length\n",
    "spotify_sample = pd.read_csv(\"../../data/raw_data/Spotify/AD_spotify_accoustic_char_250k.csv\", nrows=5000)\n",
    "\n",
    "#load the non-album matched song + artist dataset\n",
    "spotify_non_album = pd.read_csv()\n",
    "\n",
    "#load the album-matched song + artist dataset\n",
    "spotify_album = pd.read_csv()\n",
    "\n",
    "#load the chart songs dataset\n",
    "spotify_charts = pd.read_csv()\n",
    "\n",
    "#load the unmatched song + artist dataset\n",
    "spotify_unmatched_transformed = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Operations to merge above datasets\n",
    "spotify_merged = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate rows from the spotify_fetch dataframe based on isrcs\n",
    "spotify_fetch = spotify_merged.drop_duplicates(subset='ISRC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to Retrieve Chartmetric ID for an ISRC ---\n",
    "def get_chartmetric_ids(isrc):\n",
    "    endpoint = f\"/api/track/isrc/{isrc}/get-ids\"\n",
    "    try:\n",
    "        response = get_request(endpoint)\n",
    "    \n",
    "    # Log the response status code and rate limit headers\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to get Chartmetric ID for ISRC {isrc}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Expecting response[\"obj\"] to be a non-empty list\n",
    "    if response.get(\"obj\") and isinstance(response[\"obj\"], list) and len(response[\"obj\"]) > 0:\n",
    "\n",
    "        # Extract the chartmetric_ids from the first element of the list\n",
    "        cm_ids = response[\"obj\"][0].get(\"chartmetric_ids\", None)\n",
    "\n",
    "        # Check if cm_ids is a non-empty list\n",
    "        if cm_ids and isinstance(cm_ids, list) and len(cm_ids) > 0:\n",
    "            try:\n",
    "                return float(cm_ids[0])\n",
    "            \n",
    "            # Log conversion errors\n",
    "            except Exception as conv_err:\n",
    "                logging.error(f\"Conversion error for ISRC {isrc}: {conv_err}\")\n",
    "                return None\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0: ISRC = USCA29900849\n",
      "Row 0 processed: ISRC = USCA29900849 -> Chartmetric ID = 15179107.0\n",
      "Processing row 2: ISRC = NLA329980201\n",
      "Row 2 processed: ISRC = NLA329980201 -> Chartmetric ID = 13470469.0\n",
      "Processing row 5: ISRC = USWB19701765\n",
      "Row 5 processed: ISRC = USWB19701765 -> Chartmetric ID = 16032274.0\n",
      "Processing row 7: ISRC = USA2B0211419\n",
      "Row 7 processed: ISRC = USA2B0211419 -> Chartmetric ID = 26971256.0\n",
      "Processing row 8: ISRC = USRO20352402\n",
      "Row 8 processed: ISRC = USRO20352402 -> Chartmetric ID = 19413107.0\n",
      "Processing row 9: ISRC = USRC15801345\n",
      "Row 9 processed: ISRC = USRC15801345 -> Chartmetric ID = 15679785.0\n",
      "Processing row 11: ISRC = USBL10000087\n",
      "Row 11 processed: ISRC = USBL10000087 -> Chartmetric ID = 19745418.0\n",
      "Processing row 12: ISRC = US3R40500030\n",
      "Row 12 processed: ISRC = US3R40500030 -> Chartmetric ID = 42960343.0\n",
      "Processing row 13: ISRC = JPKI00309478\n",
      "Row 13 processed: ISRC = JPKI00309478 -> Chartmetric ID = 85712503.0\n",
      "Processing row 14: ISRC = GBCEL0300064\n",
      "Row 14 processed: ISRC = GBCEL0300064 -> Chartmetric ID = 12629581.0\n",
      "Processing row 16: ISRC = USA4D0010054\n",
      "Row 16 processed: ISRC = USA4D0010054 -> Chartmetric ID = 26263766.0\n",
      "Processing row 17: ISRC = USWB10302433\n",
      "Row 17 processed: ISRC = USWB10302433 -> Chartmetric ID = 16016563.0\n",
      "Processing row 20: ISRC = CAV169800737\n",
      "Row 20 processed: ISRC = CAV169800737 -> Chartmetric ID = 11209743.0\n",
      "Processing row 22: ISRC = USUMG9900708\n",
      "Row 22 processed: ISRC = USUMG9900708 -> Chartmetric ID = 15942552.0\n",
      "Processing row 23: ISRC = US2V60431513\n",
      "Row 23 processed: ISRC = US2V60431513 -> Chartmetric ID = 14554988.0\n",
      "Processing row 24: ISRC = USEP40322113\n",
      "Row 24 processed: ISRC = USEP40322113 -> Chartmetric ID = 15285227.0\n",
      "Processing row 25: ISRC = DEH259900035\n",
      "Row 25 processed: ISRC = DEH259900035 -> Chartmetric ID = 16979691.0\n",
      "Processing row 26: ISRC = USAT20104587\n",
      "Row 26 processed: ISRC = USAT20104587 -> Chartmetric ID = 15070816.0\n",
      "Processing row 28: ISRC = USLF20000233\n",
      "Row 28 processed: ISRC = USLF20000233 -> Chartmetric ID = 15483776.0\n",
      "Processing row 29: ISRC = USRS73710013\n",
      "Row 29 processed: ISRC = USRS73710013 -> Chartmetric ID = 15716132.0\n",
      "Processing row 30: ISRC = USAM19500612\n",
      "Row 30 processed: ISRC = USAM19500612 -> Chartmetric ID = 15041953.0\n",
      "Processing row 36: ISRC = USSM17200589\n",
      "Row 36 processed: ISRC = USSM17200589 -> Chartmetric ID = 15807902.0\n",
      "Processing row 37: ISRC = US3M50018421\n",
      "Row 37 processed: ISRC = US3M50018421 -> Chartmetric ID = 24461192.0\n",
      "Processing row 38: ISRC = GBAYE0200633\n",
      "Row 38 processed: ISRC = GBAYE0200633 -> Chartmetric ID = 12522934.0\n",
      "Processing row 40: ISRC = GBAJE0100089\n",
      "Row 40 processed: ISRC = GBAJE0100089 -> Chartmetric ID = 19521818.0\n",
      "Processing row 41: ISRC = USDW10022040\n",
      "Row 41 processed: ISRC = USDW10022040 -> Chartmetric ID = 15244090.0\n",
      "Processing row 42: ISRC = USVG20400197\n",
      "Row 42 processed: ISRC = USVG20400197 -> Chartmetric ID = 21372107.0\n",
      "Processing row 43: ISRC = AUAP08300015\n",
      "Row 43 processed: ISRC = AUAP08300015 -> Chartmetric ID = 10988441.0\n",
      "Processing row 44: ISRC = USLR50100055\n",
      "Row 44 processed: ISRC = USLR50100055 -> Chartmetric ID = 15492548.0\n",
      "Processing row 45: ISRC = USSM10207648\n",
      "Row 45 processed: ISRC = USSM10207648 -> Chartmetric ID = 16441727.0\n",
      "Processing row 46: ISRC = USBAS0400012\n",
      "Row 46 processed: ISRC = USBAS0400012 -> Chartmetric ID = 15114218.0\n",
      "Processing row 47: ISRC = USEE10001960\n",
      "Row 47 processed: ISRC = USEE10001960 -> Chartmetric ID = 15265382.0\n",
      "Processing row 48: ISRC = GBAKW7501028\n",
      "Row 48 processed: ISRC = GBAKW7501028 -> Chartmetric ID = 12481397.0\n",
      "Processing row 49: ISRC = USA560459552\n",
      "Row 49 processed: ISRC = USA560459552 -> Chartmetric ID = 26728232.0\n"
     ]
    }
   ],
   "source": [
    "# Ensure the 'chartmetric_ids' column exists; if not, create it and fill with None.\n",
    "if \"chartmetric_ids\" not in spotify_fetch.columns:\n",
    "    spotify_fetch[\"chartmetric_ids\"] = None\n",
    "\n",
    "# --- Main Processing with Checkpointing & Throttling ---\n",
    "checkpoint_file = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/chartmetric_track_id_checkpoints/chartmetric_ids_checkpoint.csv\"\n",
    "checkpoint_interval = 100  # save every 100 processed rows\n",
    "\n",
    "for idx, row in spotify_fetch.iterrows():\n",
    "    isrc = row['ISRC']\n",
    "    print(f\"Processing row {idx}: ISRC = {isrc}\")\n",
    "\n",
    "    # Skip if ISRC already processed (if you restart from a checkpoint)\n",
    "    if pd.notnull(spotify_fetch.at[idx, \"chartmetric_ids\"]):\n",
    "        print(f\"Row {idx} already processed. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        chartmetric_id = get_chartmetric_ids(isrc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ISRC {isrc} at row {idx}: {e}\")\n",
    "        logging.error(f\"Error processing ISRC {isrc} at row {idx}: {e}\")\n",
    "        chartmetric_id = None\n",
    "\n",
    "    spotify_fetch.at[idx, \"chartmetric_ids\"] = chartmetric_id\n",
    "    print(f\"Row {idx} processed: ISRC = {isrc} -> Chartmetric ID = {chartmetric_id}\")\n",
    "    logging.info(f\"Processed row {idx}, ISRC {isrc}: Chartmetric ID = {chartmetric_id}\")\n",
    "\n",
    "    # Optional: sleep briefly between requests to smooth out request rate.\n",
    "    time.sleep(0.3)  # adjust as needed\n",
    "\n",
    "    # Save a checkpoint periodically\n",
    "    if idx % checkpoint_interval == 0 and idx > 0:\n",
    "        spotify_fetch.to_csv(checkpoint_file, index=False)\n",
    "        print(f\"Checkpoint saved at row {idx}\")\n",
    "        logging.info(f\"Checkpoint saved at row {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the object type into float \n",
    "spotify_fetch[\"chartmetric_ids\"] = pd.to_numeric(spotify_fetch[\"chartmetric_ids\"], errors='coerce')\n",
    "\n",
    "#transform into integer\n",
    "spotify_fetch[\"chartmetric_ids\"] = spotify_fetch[\"chartmetric_ids\"].astype(\"Int64\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retain only the rows with unique chartmetric_ids\n",
    "spotify_fetch_unique = spotify_fetch.drop_duplicates(subset=\"chartmetric_ids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the files needs to take into account the version that I have already saved in the past as I am not able to run the code remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Define the final file path\n",
    "# NEED TO CHECK:  the suffix\n",
    "filepath = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/\"\n",
    "\n",
    "#give the file a name\n",
    "#file_name = \"chartmetric_ids_fetch.csv\"\n",
    "\n",
    "#paste filepath and file_name together and call the variable final_filepath\n",
    "\n",
    "final_filepath = os.path.join(filepath, file_name)\n",
    "\n",
    "# Save the dataframe to the final_filepath\n",
    "spotify_fetch_unique.to_csv(final_filepath, index=False)\n",
    "print(f\"Saved file as: {final_filepath}\")\n",
    "\n",
    "# Log the final message in the log file\n",
    "logging.info(\"Completed processing all ISRC codes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_data_chartmetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
