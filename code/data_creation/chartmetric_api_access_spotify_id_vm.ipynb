{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t# Author: Alexander Staub\n",
    "\t## Last changed: 2025.02.13\n",
    "\t## Purpose: Getting the chartmetrics IDs for a list of songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing packages\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the logging of the errors\n",
    "logging.basicConfig(\n",
    "    filename='chartmetric_api.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s: %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define API host and your refresh token\n",
    "HOST = 'https://api.chartmetric.com'\n",
    "with open(\"chartmetric_refresh_token.txt\", \"r\") as f:\n",
    "    REFRESH_TOKEN = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve an access token using the refresh token\n",
    "token_response = requests.post(f'{HOST}/api/token', json={'refreshtoken': REFRESH_TOKEN})\n",
    "\n",
    "# Check if the token was retrieved successfully\n",
    "if token_response.status_code != 200:\n",
    "\n",
    "    # Log the error and raise an exception\n",
    "    logging.error(f\"Token retrieval error: {token_response.status_code}\")\n",
    "    raise Exception(f\"Error: received {token_response.status_code} from /api/token\")\n",
    "\n",
    "# Extract the access token from the response\n",
    "access_token = token_response.json()['token']\n",
    "\n",
    "# Define the headers for the API requests\n",
    "headers = {'Authorization': f'Bearer {access_token}'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the get_request\n",
    "\n",
    "Robust request logic that:\n",
    "- backs off for a max of 26 hours in retries\n",
    "- logs all erros it encounters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Robust get_request Function ---\n",
    "def get_request(endpoint, params=None, max_retries=5):\n",
    "    backoff = 1  # initial backoff in seconds (used if header data is missing)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(f\"{HOST}{endpoint}\", headers=headers, params=params)\n",
    "        except Exception as ex:\n",
    "            logging.error(f\"Network error on attempt {attempt+1} for {endpoint}: {ex}\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "            continue\n",
    "\n",
    "# Log the response status code and rate limit headers\n",
    "        logging.info(f\"Request to {endpoint} returned {response.status_code}. RateLimit headers: {response.headers}\")\n",
    "\n",
    "# Check if the response status code is 200\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "\n",
    "# Handle different types of errors\n",
    "# 401: Token may have expired; refresh it\n",
    "        elif response.status_code == 401:\n",
    "            # Token may have expired; refresh it\n",
    "            logging.warning(f\"401 error for {endpoint}. Refreshing token.\")\n",
    "            token_response = requests.post(f'{HOST}/api/token', json={'refreshtoken': REFRESH_TOKEN})\n",
    "            if token_response.status_code != 200:\n",
    "                logging.error(f\"Token refresh failed: {token_response.status_code}\")\n",
    "                raise Exception(f\"Token refresh failed with status {token_response.status_code}\")\n",
    "            new_token = token_response.json()['token']\n",
    "            headers['Authorization'] = f'Bearer {new_token}'\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "\n",
    "# 429: Rate limit exceeded; wait and retry\n",
    "        elif response.status_code == 429:\n",
    "            # Rate limit exceeded.\n",
    "            reset_timestamp = response.headers.get(\"X-RateLimit-Reset\")\n",
    "            if reset_timestamp:\n",
    "                # Wait until the time provided by the API\n",
    "                sleep_time = int(reset_timestamp) - int(time.time())\n",
    "                if sleep_time < 0:\n",
    "                    sleep_time = backoff\n",
    "            else:\n",
    "                # No wait time provided by the API; compute one that totals 26 hours over all retries.\n",
    "                total_wait_limit = 26 * 3600  # total wait time in seconds (26 hours)\n",
    "                # Sum exponential weights for remaining attempts: for i from current attempt to max_retries-1\n",
    "                remaining_weights = sum(2 ** i for i in range(attempt, max_retries))\n",
    "                # Use the weight for the current attempt to assign a fraction of the total wait.\n",
    "                sleep_time = total_wait_limit * (2 ** attempt / remaining_weights)\n",
    "            logging.warning(f\"429 error for {endpoint}. Sleeping for {sleep_time} seconds (attempt {attempt+1}/{max_retries}).\")\n",
    "            time.sleep(sleep_time)\n",
    "            backoff *= 2\n",
    "\n",
    "# 500: Server error; wait and retry\n",
    "        elif response.status_code >= 500:\n",
    "            logging.warning(f\"Server error {response.status_code} for {endpoint}. Retrying after {backoff} seconds.\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "\n",
    "        else:\n",
    "            logging.error(f\"Error {response.status_code} for {endpoint}: {response.text}\")\n",
    "            raise Exception(f\"Error: received {response.status_code} from {endpoint}\")\n",
    "\n",
    "# If the loop completes without returning, raise an exception\n",
    "    raise Exception(f\"Max retries exceeded for endpoint {endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use isrc to access song characteristics:\n",
    "- load in a subset of around 5000 of the spotify songs with spotify IDs\n",
    "- Loop over them to get the chartmetric IDs and any other further information accessible\n",
    "- use chartmetric IDs to get the information of relevance\n",
    "- setup code in a way that allows us to get information for our relevant songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the first 8000 lines from the file \"AD_spotify_accoustic_char_250k.csv\" in the data/raw_data/Spotify folder and call it spotify_fetch\n",
    "# the data directory is 2 directories down from the current directory\n",
    "# it should be a pandas dataframe and the csv has headers\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#sample\n",
    "spotify_sample = pd.read_csv(\"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/Spotify/1980_2000_songs_artists/musicbrainz_spotify_combined_track_artist_final.csv\", nrows=5000)\n",
    "\n",
    "#load the -matched song + artist dataset\n",
    "#spotify_album = pd.read_csv(\"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/Spotify/1980_2000_songs_artists/musicbrainz_spotify_combined_track_artist_final.csv\")\n",
    "\n",
    "#load the chart songs dataset\n",
    "#spotify_charts = pd.read_csv()\n",
    "\n",
    "#load the unmatched song + artist dataset\n",
    "#spotify_unmatched_transformed = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Operations to merge above datasets\n",
    "spotify_merged = spotify_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate rows from the spotify_fetch dataframe based on isrcs\n",
    "spotify_fetch = spotify_merged.drop_duplicates(subset='spotify_isrc', keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to Retrieve Chartmetric ID for an ISRC ---\n",
    "def get_chartmetric_ids(isrc):\n",
    "    endpoint = f\"/api/track/isrc/{isrc}/get-ids\"\n",
    "    try:\n",
    "        response = get_request(endpoint)\n",
    "    \n",
    "    # Log the response status code and rate limit headers\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to get Chartmetric ID for ISRC {isrc}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Expecting response[\"obj\"] to be a non-empty list\n",
    "    if response.get(\"obj\") and isinstance(response[\"obj\"], list) and len(response[\"obj\"]) > 0:\n",
    "\n",
    "        # Extract the chartmetric_ids from the first element of the list\n",
    "        cm_ids = response[\"obj\"][0].get(\"chartmetric_ids\", None)\n",
    "\n",
    "        # Check if cm_ids is a non-empty list\n",
    "        if cm_ids and isinstance(cm_ids, list) and len(cm_ids) > 0:\n",
    "            try:\n",
    "                return float(cm_ids[0])\n",
    "            \n",
    "            # Log conversion errors\n",
    "            except Exception as conv_err:\n",
    "                logging.error(f\"Conversion error for ISRC {isrc}: {conv_err}\")\n",
    "                return None\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'chartmetric_ids' column exists; if not, create it and fill with None.\n",
    "if \"chartmetric_ids\" not in spotify_fetch.columns:\n",
    "    spotify_fetch[\"chartmetric_ids\"] = None\n",
    "\n",
    "# --- Main Processing with Checkpointing & Throttling ---\n",
    "checkpoint_file = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/incidental/chartmetric/chartmetric_track_id_checkpoints/chartmetric_ids_checkpoint.csv\"\n",
    "checkpoint_interval = 100  # save every 100 processed rows\n",
    "\n",
    "for idx, row in spotify_fetch.iterrows():\n",
    "    isrc = row['spotify_isrc']\n",
    "    print(f\"Processing row {idx}: ISRC = {isrc}\")\n",
    "\n",
    "    # Skip if ISRC already processed (if you restart from a checkpoint)\n",
    "    if pd.notnull(spotify_fetch.at[idx, \"chartmetric_ids\"]):\n",
    "        print(f\"Row {idx} already processed. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        chartmetric_id = get_chartmetric_ids(isrc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ISRC {isrc} at row {idx}: {e}\")\n",
    "        logging.error(f\"Error processing ISRC {isrc} at row {idx}: {e}\")\n",
    "        chartmetric_id = None\n",
    "\n",
    "    spotify_fetch.at[idx, \"chartmetric_ids\"] = chartmetric_id\n",
    "    print(f\"Row {idx} processed: ISRC = {isrc} -> Chartmetric ID = {chartmetric_id}\")\n",
    "    logging.info(f\"Processed row {idx}, ISRC {isrc}: Chartmetric ID = {chartmetric_id}\")\n",
    "\n",
    "    # Optional: sleep briefly between requests to smooth out request rate.\n",
    "    time.sleep(0.3)  # adjust as needed\n",
    "\n",
    "    # Save a checkpoint periodically\n",
    "    if idx % checkpoint_interval == 0 and idx > 0:\n",
    "        spotify_fetch.to_csv(checkpoint_file, index=False)\n",
    "        print(f\"Checkpoint saved at row {idx}\")\n",
    "        logging.info(f\"Checkpoint saved at row {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the object type into float \n",
    "spotify_fetch[\"chartmetric_ids\"] = pd.to_numeric(spotify_fetch[\"chartmetric_ids\"], errors='coerce')\n",
    "\n",
    "#transform into integer\n",
    "spotify_fetch[\"chartmetric_ids\"] = spotify_fetch[\"chartmetric_ids\"].astype(\"Int64\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retain only the rows with unique chartmetric_ids\n",
    "spotify_fetch_unique = spotify_fetch.drop_duplicates(subset=\"chartmetric_ids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the files needs to take into account the version that I have already saved in the past as I am not able to run the code remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Define the final file path\n",
    "# NEED TO CHECK:  the suffix\n",
    "filepath = \"//bigdata.wu.ac.at/delpero/Data_alexander/data/raw_data/chartmetric/chartmetric_ids\"\n",
    "\n",
    "#give the file a name\n",
    "#file_name = \"chartmetric_ids_sample.csv\"\n",
    "\n",
    "#paste filepath and file_name together and call the variable final_filepath\n",
    "\n",
    "final_filepath = os.path.join(filepath, file_name)\n",
    "\n",
    "# Save the dataframe to the final_filepath\n",
    "spotify_fetch_unique.to_csv(final_filepath, index=False)\n",
    "print(f\"Saved file as: {final_filepath}\")\n",
    "\n",
    "# Log the final message in the log file\n",
    "logging.info(\"Completed processing all ISRC codes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_data_chartmetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
