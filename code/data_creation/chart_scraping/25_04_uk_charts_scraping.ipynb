{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e65e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "author: Alexander Staub\n",
    "Date: 2025-04-21\n",
    "Description: Script for scraper of \"officialcharts.com\", UK charts \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb47988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scrape Official Charts (UK) – Singles & Albums, 1980-2000\n",
    "========================================================\n",
    "Outputs a single CSV with the columns\n",
    "\n",
    "    chart_type        \"singles\" | \"albums\"\n",
    "    chart_week_start  YYYY-MM-DD (Sunday shown in the URL)\n",
    "    position          1-100 (or 1-50 if the page shows only 50)*\n",
    "    last_week         integer or NaN\n",
    "    weeks_on_chart    integer\n",
    "    song_title\n",
    "    artist_name\n",
    "    song_url          URL of the title link (for later label lookup)\n",
    "\n",
    "The script is **checkpoint-aware**: if you stop it midway you can rerun it\n",
    "and it will skip the weeks already in the CSV.\n",
    "\n",
    "Requirement summary\n",
    "-------------------\n",
    "1) Two separate scrapes handled by the same script (`chart_type` param)  \n",
    "2) Every available position each week (page displays up to 100)  \n",
    "3) `song_url` retained instead of fetching the label now  \n",
    "4) Produces **one big CSV** in `data/` (relative path)  \n",
    "5) Works locally & remotely – all paths are relative  \n",
    "6) Robust to interruptions, rate-limits, and missing weeks  \n",
    "7) Random 0.5-2 s delay between *HTTP* requests\n",
    "\n",
    "© 2025 – academic-use only.  ↝ MIT licence if you wish.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be34b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing required packages\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3116947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# CONSTANTS & CONFIG\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "BASE_URL = \"https://www.officialcharts.com/charts\"\n",
    "CHART_IDS = {\n",
    "    \"singles\": 7501,  # /singles-chart/{date}/7501/\n",
    "    \"albums\": 7502,   # /albums-chart/{date}/7502/\n",
    "}\n",
    "\n",
    "# Go three levels up from the current working directory\n",
    "base_dir = Path.cwd().parents[2]\n",
    "CSV_PATH = base_dir / \"data\" / \"raw_data\" / \"country_chart_data\" / \"uk_charts_1980_2000.csv\"\n",
    "CSV_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"chart-research/1.0 (+https://github.com/YOUR_GH_HANDLE)\",\n",
    "    \"Accept-Language\": \"en-GB,en;q=0.9\",\n",
    "}\n",
    "\n",
    "START_DATE = date(1979, 12, 30)   # first Sunday in 1980\n",
    "END_DATE   = date(1999, 12, 26) # last Sunday in 2000\n",
    "\n",
    "REQUEST_DELAY_RANGE = (1, 3.0)  # polite crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f698e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# HTTP helpers\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "\n",
    "@retry(\n",
    "    reraise=True,\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_random_exponential(multiplier=1, max=20),\n",
    ")\n",
    "def fetch(url: str) -> str:\n",
    "    \"\"\"GET a URL with retries & exponential back-off (tenacity).\"\"\"\n",
    "    response = session.get(url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db74941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# Core scraping logic\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def weekly_dates(start: date, end: date) -> Iterable[date]:\n",
    "    \"\"\"Yield Sunday dates inclusive.\"\"\"\n",
    "    d = start\n",
    "    one_week = timedelta(days=7)\n",
    "    while d <= end:\n",
    "        yield d\n",
    "        d += one_week\n",
    "\n",
    "\n",
    "def parse_chart(html: str) -> List[dict]:\n",
    "    \"\"\"Extract rows from a weekly chart page.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # list view is the default – every entry lives inside <div class=\"chart-positions\">\n",
    "    # but some historical pages use <div class=\"chart-positions-list\">.\n",
    "    entries = soup.select(\"div.chart-positions > div\") or soup.select(\n",
    "        \"div.chart-positions-list > div\"\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for e in entries:\n",
    "        pos_tag = e.select_one(\".position\")\n",
    "        title_tag = e.select_one(\".title\")\n",
    "        artist_tag = e.select_one(\".artist\")\n",
    "        meta_tag = e.select_one(\".meta\")  # contains LW & Weeks\n",
    "\n",
    "        if not all((pos_tag, title_tag, artist_tag, meta_tag)):\n",
    "            # malformed entry – skip quietly\n",
    "            continue\n",
    "\n",
    "        # meta text example: \"LW: 3 | Peak: 1 | Weeks: 7\"\n",
    "        meta_text = meta_tag.get_text(strip=True).lower()\n",
    "        parts = {kv.split(\":\")[0]: kv.split(\":\")[1] for kv in meta_text.split(\",\")}\n",
    "        last_week = parts.get(\"lw\", \"\").strip() or None\n",
    "        weeks_on = parts.get(\"weeks\", \"\").strip() or None\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"position\": int(pos_tag.get_text(strip=True)),\n",
    "                \"song_title\": title_tag.get_text(strip=True),\n",
    "                \"artist_name\": artist_tag.get_text(strip=True),\n",
    "                \"last_week\": int(last_week) if last_week and last_week.isdigit() else None,\n",
    "                \"weeks_on_chart\": int(weeks_on) if weeks_on and weeks_on.isdigit() else None,\n",
    "                \"song_url\": title_tag.find(\"a\")[\"href\"] if title_tag.find(\"a\") else None,\n",
    "            }\n",
    "        )\n",
    "    return rows\n",
    "\n",
    "\n",
    "def scrape_chart_for_week(chart_type: str, week: date) -> pd.DataFrame | None:\n",
    "    chart_id = CHART_IDS[chart_type]\n",
    "    url = f\"{BASE_URL}/{chart_type}-chart/{week:%Y%m%d}/{chart_id}/\"\n",
    "    try:\n",
    "        html = fetch(url)\n",
    "    except requests.HTTPError as exc:\n",
    "        print(f\"[WARN] HTTP error {exc.response.status_code} for {url} – skipping\")\n",
    "        return None\n",
    "    except Exception as exc:\n",
    "        print(f\"[ERROR] {exc} – skipping {url}\")\n",
    "        return None\n",
    "\n",
    "    rows = parse_chart(html)\n",
    "    if not rows:  # page present but empty (rare)\n",
    "        print(f\"[WARN] No rows extracted from {url}\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.insert(0, \"chart_week_start\", pd.Timestamp(week))\n",
    "    df.insert(0, \"chart_type\", chart_type)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83511f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# Progress / resume helpers\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def already_scraped_weeks() -> set[tuple[str, str]]:\n",
    "    \"\"\"Read existing CSV (if any) and return {(chart_type, ISO week-start)}.\"\"\"\n",
    "    if not CSV_PATH.exists():\n",
    "        return set()\n",
    "    df = pd.read_csv(CSV_PATH, usecols=[\"chart_type\", \"chart_week_start\"])\n",
    "    return {(row.chart_type, row.chart_week_start) for row in df.itertuples()}\n",
    "\n",
    "\n",
    "def append_to_csv(df: pd.DataFrame) -> None:\n",
    "    header = not CSV_PATH.exists()\n",
    "    df.to_csv(CSV_PATH, mode=\"a\", index=False, header=header, quoting=csv.QUOTE_MINIMAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b424fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# Main driver\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def main() -> None:\n",
    "    done = already_scraped_weeks()\n",
    "    total_weeks = len(list(weekly_dates(START_DATE, END_DATE)))\n",
    "    for chart_type in (\"singles\", \"albums\"):\n",
    "        print(f\"\\n→  Starting chart: {chart_type.upper()}\")\n",
    "        for i, week in enumerate(weekly_dates(START_DATE, END_DATE), 1):\n",
    "            key = (chart_type, str(week))\n",
    "            if key in done:\n",
    "                continue\n",
    "\n",
    "            print(\n",
    "                f\"[{chart_type[:1].upper()}] \"\n",
    "                f\"{week}  ({i}/{total_weeks}) … \",\n",
    "                end=\"\",\n",
    "                flush=True,\n",
    "            )\n",
    "            df = scrape_chart_for_week(chart_type, week)\n",
    "            if df is not None:\n",
    "                append_to_csv(df)\n",
    "                print(f\"✓  {len(df)} rows\")\n",
    "            time.sleep(random.uniform(*REQUEST_DELAY_RANGE))\n",
    "\n",
    "    print(\"\\nAll done – dataset saved to\", CSV_PATH.relative_to(Path.cwd()))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_data_chartmetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
